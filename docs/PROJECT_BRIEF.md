Russian version
Newral: Архитектура платформы распределённых вычислений нового поколения
Цели и описание проекта
Newral – это платформа распределённых вычислений нового поколения, разрабатываемая как универсальная среда для выполнения масштабных вычислительных задач на множестве узлов. Цель проекта – предоставить гибкую и надёжную инфраструктуру, которая объединяет вычислительные ресурсы разных участников (узлов) для решения сложных задач, от научных вычислений до обработки больших данных. Платформа ориентирована на устранение ограничений предыдущих поколений распределённых систем (например, проектов добровольных вычислений) за счёт внедрения современных подходов: микросервисной архитектуры, AI-оркестрации, механизмов доверия и вознаграждений. Newral призвана обеспечить безопасность, надёжность и эффективность распределённых вычислений, формируя доверительную среду для сотрудничества узлов[1].
Ключевые отличия Newral от традиционных платформ заключаются в акценте на интеллектуальном оркестраторе с элементами искусственного интеллекта, а также в встроенных механизмах проверки результатов и репутации узлов. Проект нацелен на поддержку как простых параллельных задач, так и сложных workflows, представленных в виде DAG (ориентированных ацикличных графов). Благодаря этому Newral сможет обслуживать широкий спектр проектов – от поисков математических объектов (например, проекты BPSW-Hunter и Кармайл) до распределённого обучения моделей ИИ. Платформа спроектирована как микросервисная с независимыми компонентами, что упрощает масштабирование и сопровождение системы. В целом, Newral стремится объединить передовые технологии распределённых систем и машинного интеллекта, чтобы создать надёжную и производительную вычислительную сеть нового поколения.
Архитектура платформы
Микросервисная архитектура: Архитектура Newral основана на микросервисах – наборе небольших автономных сервисов, каждый из которых реализует отдельную функцию системы. Подход микросервисов обеспечивает гибкость разработки и развертывания: каждый компонент можно масштабировать и обновлять независимо от других[2]. В Newral выделены следующие основные компоненты (сервисы):
•	Оркестратор задач (AI-оркестратор): центральный сервис, отвечающий за управление заданиями. Оркестратор получает поступающие задачи, разбивает их при необходимости на подзадачи, распределяет их между вычислительными узлами и отслеживает выполнение. Он же занимается учётом зависимостей задач (DAG) – гарантирует, что задачи запускаются после выполнения всех необходимых предшественников.
•	Вычислительные узлы (агенты): узлы, предоставляющие свои ресурсы (CPU/GPU, память и пр.) для выполнения задач. На каждом узле работает клиентское приложение-агент, которое получает задания от оркестратора, исполняет их в изолированной среде и возвращает результаты. Узлы могут иметь разную конфигурацию и производительность, поэтому оркестратор учитывает возможности каждого узла (тип CPU/GPU, объём памяти и т.д.) при распределении нагрузки.
•	Сервис доверия и безопасности (система Dr. Mann#n): компонент, реализующий политику доверия к узлам. Он хранит и обновляет показатели репутации узлов, выдаёт сертификаты доверенного вычислителя и принимает решения о необходимости дополнительной проверки результатов. Название Dr. Mann#n условное – эта система выполняет роль «врача» сети, диагностируя поведение узлов и выявляя аномалии или недобросовестные действия.
•	База данных и хранилище: централизованные хранилища для данных платформы. Реляционная база данных (PostgreSQL) используется для хранения структурированной информации – регистрационных данных узлов и пользователей, заданий, статусов выполнения, репутационных показателей и т.д. Объектное хранилище применяется для больших данных: входных наборов, выходных результатов, контрольных точек вычислений. Разделение хранения метаданных и больших объёмов данных повышает надёжность и масштабируемость: метаданные находятся в БД, а крупные бинарные объекты хранятся отдельно, в реплицируемом хранилище (например, совместимом с S3). Такая архитектура делает вычислительные узлы состояния не содержащими – в случае сбоя узла его задачи могут быть переназначены, а необходимые данные извлечены из хранилища согласно метаданным в БД[3].
•	Шина событий (Kafka): коммуникационный слой платформы, основанный на Apache Kafka. Все сервисы обмениваются сообщениями через общую шину – события. Оркестратор публикует события о новых задачах, статусах выполнения; узлы публикуют события о результатах, метриках и состоянии; сервис доверия получает события для обновления репутации и инициирования проверок. Использование Kafka обеспечивает слабую связность между компонентами и масштабируемость системы: сервисы обмениваются данными асинхронно через топики Kafka, что позволяет им развиваться независимо и выдерживать высокую нагрузку[4]. Кроме того, Kafka надёжно сохраняет сообщения в распределённом журнале, что повышает устойчивость к сбоям – ни одно событие (например, результат задачи) не будет утеряно при временном отключении сервиса[5].
•	API-шлюз и интерфейсы: для взаимодействия внешних пользователей или приложений с платформой предусмотрен API-шлюз. Через него пользователи могут добавлять новые задачи, получать информацию о прогрессе, просматривать метрики. Шлюз обеспечивает аутентификацию, авторизацию, а также агрегирует несколько внутренних сервисов в единый внешний интерфейс. Кроме API, может быть реализован веб-интерфейс или клиентские приложения для мониторинга и управления задачами.
Микросервисная организация компонентов повышает отказоустойчивость системы – сбой одного сервиса (например, узла) не выводит из строя всю платформу, так как оркестратор может перенаправить работу на другие узлы. Каждый сервис развернут независимо, общение происходит через чётко определённые API или очереди событий, что также облегчает развитие платформы. Для управления жизненным циклом микросервисов (развёртывание, масштабирование, перезапуск при сбоях) планируется использовать контейнеризацию и оркестрацию контейнеров (например, Kubernetes)[6]. Это позволит автоматически размещать сервисы на доступных серверах, следить за их здоровьем и динамически масштабировать подсистемы под нагрузку.
Узлы и их роли: В текущей реализации предполагается, что все вычислительные узлы выполняют одинаковую роль – получают задачи на выполнение. Тем не менее, некоторые узлы могут специализироваться: например, узлы-валидаторы, которые участвуют в проверке результатов (об этом подробнее в разделе валидатора). Также возможна дифференциация по типам ресурсов – например, узлы с GPU могут получать задачи, требующие ускорений на графических процессорах. Оркестратор регистрирует характеристики каждого узла (количество ядер, наличие GPU, пропускную способность сети и т.п.) и распределяет задачи с учётом этих параметров. Благодаря этому платформа эффективно задействует разнородные ресурсы: высокопроизводительные узлы берут больше или более тяжёлые задания, а узлы послабее не перегружаются несоразмерной работой. Это помогает достичь оптимального баланса загрузки и ускорить общее выполнение распределённых задач.
Режимы AI-оркестратора
Одной из инновационных особенностей Newral является интеллектуальный оркестратор задач, способный работать в различных режимах автоматизации на основе искусственного интеллекта. Оркестратор с элементами ИИ может самостоятельно принимать решения по планированию и распределению задач, однако уровень автономности регулируется настройками. Всего предусмотрено четыре режима работы оркестратора:
•	AI Off (ИИ выключен): в этом режиме функции ИИ-анализа отключены. Оркестратор действует по предопределённым правилам или сценариям, либо же управляется вручную администраторами. Все решения о планировании, назначении задач узлам, перераспределении нагрузки принимаются на основе статических алгоритмов (например, кругового распределения, FIFO-очереди) или человеком. Данный режим актуален для первоначального этапа (MVP), когда система ещё обучается или когда необходимо полное ручное управление для отладки и доверия. Он гарантирует предсказуемое поведение без вмешательства ИИ.
•	Advisory (Рекомендательный режим): в этом режиме оркестратор использует алгоритмы ИИ в консультативном режиме. ИИ-модуль анализирует состояние системы (нагрузку узлов, статистику успешности, время выполнения задач) и предлагает оптимизации: например, рекомендует перенести часть задач на другие узлы для балансировки, советует приоритеты выполнения или выделение ресурсов. Однако финальное решение остаётся за человеком-оператором или за детерминированной логикой. Проще говоря, ИИ выступает советником, генерируя подсказки и прогнозы (например, прогноз времени завершения крупной задачи, выявление потенциальных узких мест), но автоматически не изменяет планировку. Этот режим повышает эффективность управления, не снимая контроля с человека – оператор видит рекомендации и может следовать им или игнорировать. Advisory-режим позволяет постепенно внедрять ИИ в процесс оркестрации, проверяя корректность его предложений.
•	Partial (Частичная автоматизация): частично автоматизированный режим, при котором оркестратор самостоятельно выполняет определённый ограниченный набор действий на основе ИИ, остальные же решения остаются под контролем человека или фиксированных правил. Например, оркестратор может автоматически балансировать нагрузку между узлами (перераспределять задачи, если видит, что какой-то узел перегружен) или автоматически перезапускать задачи, завершившиеся с ошибкой, на других узлах. Однако, при этом он может не трогать критические аспекты – скажем, не будет сам выбирать, какие проекты или задания запускать в первую очередь (это всё ещё решает администратор). Partial-режим – промежуточный шаг: ИИ берет на себя рутинные операции (как автопилот на некоторых этапах), но стратегические решения контролируются человеком. Этот уровень автоматизации разгружает операторов от множества мелких действий и ошибок, ускоряя реакцию системы на типичные ситуации (например, отключение узла, внезапное увеличение очереди задач и т.д.).
•	Full (Полная автоматизация): в полном AI-режиме оркестратор обладает максимальной автономностью. Он применяет алгоритмы машинного обучения и эвристики для всех ключевых решений: планирование очереди задач, выбор оптимальных узлов с учётом их характеристик и текущей загрузки, динамическое масштабирование (подключение новых узлов при необходимости), перестановка приоритетов задач в реальном времени и пр. Практически вся оркестрация доверена ИИ, без необходимости ручного вмешательства. В этом режиме система способна самостоятельно адаптироваться к изменению условий – например, обученный на исторических данных ИИ прогнозирует время выполнения заданий и распределяет ресурсы так, чтобы минимизировать общее время или затраты. Full-режим требует высокого уровня доверия к алгоритмам ИИ, поэтому планируется к внедрению после тщательного тестирования и накопления данных о работе системы в предыдущих режимах. При полном ИИ-управлении Newral сможет в режиме реального времени реагировать на события и оптимизировать производительность комплекса без участия человека.
Переход между режимами гибко настраивается. На этапе запуска системы ожидается использование режимов AI Off или Advisory, когда администраторы могут контролировать и верифицировать решения оркестратора. По мере роста уверенности в моделях ИИ, можно постепенно включать Partial и затем Full режим, получая всё больше выгоды от автоматизации. Таким образом, архитектура Newral предполагает эволюцию оркестрации – от полностью ручной к полностью интеллектуальной, с промежуточными шагами для проверки и настройки. Это позволяет внедрять ИИ безопасно, не жертвуя надёжностью работы. В любом случае, независимо от режима, оркестратор выполняет базовые обязанности: отслеживает прогресс задач, управляет потоками данных между компонентами, реагирует на сбои. Интеллектуальные функции лишь добавляют оптимизацию и адаптивность. В итоге, многоуровневый AI-оркестратор даёт возможность использовать Newral как в консервативном сценарии (ручное управление), так и в ультрасовременном (полностью автономное распределение вычислений).
Безопасность и доверие (система Dr. Mann#n)
В открытой распределённой системе критически важно обеспечить доверие между участниками и защиту от недобросовестного поведения. Безопасность и доверие в Newral реализуются на нескольких уровнях, объединённых в систему под кодовым названием Dr. Mann#n. Эта система отвечает за проверку подлинности узлов, поддержание репутации, выявление аномалий и обеспечение целостности вычислений.
Аутентификация и защита коммуникаций: Каждый узел, подключающийся к платформе, проходит регистрацию и аутентификацию. Используются криптографические сертификаты или токены, чтобы гарантировать, что к оркестратору подключается известный системе участник, а не злоумышленник. Вся связь между компонентами (узлами, оркестратором, хранилищем) шифруется (например, с использованием TLS) для предотвращения перехвата или подделки данных. Таким образом, уже на транспортном уровне обеспечивается защита от атаки типа man-in-the-middle и от несанкционированного доступа.
Модель доверия к узлам: Система Dr. Mann#n внедряет репутационную модель доверия. Каждому узлу присваивается метрика доверия (репутация), которая динамически обновляется на основе поведения узла и результатов его работы. Узел, успешно и честно вычисляющий задачи, получает повышение репутации; если же узел выдал ошибочные результаты, задерживает выполнение или уличён в попытке обмануть систему, его репутационный рейтинг понижается. Такая модель стимулирует честное поведение, награждая надёжные узлы и наказывая нечестных[7]. Репутация каждого узла хранится в базе и влияет на решения оркестратора: задания критичной важности, требующие высокой точности, будут доверяться только узлам с высокой репутацией, тогда как узлам с низкой репутацией поручаются менее важные или дублирующие задачи (для проверки).
Мониторинг и аномалия: Компонент Dr. Mann#n постоянно мониторит показатели работы узлов: время выполнения задач, частоту ошибок, отклонение результатов от ожиданий и т.п. На основе этих данных (возможно, с применением методов машинного обучения) выявляются подозрительные активности. Например, если узел регулярно возвращает результаты, расходящиеся с другими или значительно занижает время вычислений (что может указывать на ложные быстрые ответы), система помечает такой узел как потенциально нечестный. Могут использоваться статистические методы и пороговые значения для автоматического выявления аномалий в поведении узлов. Узлы, поведение которых выходит за границы допустимого, переводятся в режим пониженного доверия: их результаты обязательно валидируются независимым способом (через дублирование или аудит), а в случае подтверждения недобросовестности такие узлы могут быть исключены из платформы.
Сертификация узлов: Узлы, долгое время демонстрирующие безупречную работу, могут получать статус доверенных. Система Dr. Mann#n выдаёт сертификаты доверия – своего рода маркировку узла как проверенного вычислителя. Например, после n успешно выполненных заданий без единой ошибки узел может получить сертификат Trusted Node Level 1, который позволит оркестратору назначать ему задачи без дополнительной репликации результатов. Сертификация многоуровневая: на высших уровнях узел допускается к самым чувствительным задачам. Сертификаты регулярно пересматриваются: если даже доверенный узел вдруг дал сбойный результат, статус может быть снижен до прохождения повторной проверки.
Криптографическая целостность кода и данных: Безопасность включает и гарантии, что вычисляемый узлами код не был подменён и данные не искажены. Поэтому Newral использует контроль целостности загрузок и цифровую подпись приложений. Когда узел получает от сервера код задачи (исполняемый файл или скрипт), клиентское приложение проверяет его хэш и подпись. Каждый бинарный файл подписан разработчиками проекта или администраторами с использованием приватного ключа, а узлы хранят соответствующий публичный ключ для проверки. Такой подход аналогичен BOINC, где хэш и подпись кода гарантируют, что узел выполняет именно авторизованное приложение[8]. Это защищает узлы от исполнения потенциально вредоносного кода, так как любые изменения файла нарушат подпись и файл будет отвергнут. Кроме того, перед отправкой результатов обратно, узел вычисляет контрольные суммы выходных данных, чтобы убедиться в отсутствии повреждений при пересылке.
Доверие через консенсус: В системе может применяться и механизм доверия, похожий на консенсус. Например, для особо важных вычислений результат признаётся корректным только если его подтвердили несколько узлов либо если валидатор (отдельный узел или алгоритм) пришёл к тому же результату. Хотя это связано с дублированием вычислений, платформа стремится минимизировать подобные случаи (подробнее об этом – в разделе про валидаторы). Однако сам принцип – «доверься, но проверяй» – заложен в архитектуру безопасности. Узлам с высокой репутацией доверяют, но контроль выборочных результатов остаётся, особенно для новых узлов. Благодаря этому достигается баланс между эффективностью и надёжностью.
Защита от сетевых атак и сбоев: Система безопасности предусматривает также защиту от DDoS-атак и переполнения: оркестратор лимитирует число задач от одного узла-источника, применяет токены с шифрованием для контроля загрузки результатов (например, узел получает ограниченный по размеру токен на выгрузку данных, чтобы злоумышленник не залил гигабайты мусора)[9]. При взаимодействии узла с сервером используются временные ограничения и бэкоффы – в случае ошибки связи повторные попытки делаются с экспоненциальной задержкой, чтобы не перегружать сеть[10]. Эти меры, хотя и технические, тоже влияют на доверие: узлы, корректно взаимодействующие с сервером, считаются надёжными участниками сети.
В совокупности система Dr. Mann#n обеспечивает фундамент доверия в распределённой среде. Она создает условия, при которых узлы могут сотрудничать, не опасаясь злоупотреблений со стороны друг друга[11]. За счёт репутационных механизмов и проверок достигается высокая надёжность и безопасность операций: только проверенные узлы выполняют ключевые вычисления, а вредоносные или сбойные узлы своевременно выявляются и изолируются. Это повышает общее доверие пользователей к платформе и стимулирует добросовестных участников продолжать работу. Без такого уровня доверия платформа распределённых вычислений была бы уязвима к ошибкам и мошенничеству, поэтому реализация безопасности и доверия является центральным элементом архитектуры Newral.
Репутация и вознаграждения
Система репутации тесно связана с механизмом доверия и служит для мотивации узлов работать честно и эффективно. Репутация узлов в Newral представляет собой числовой рейтинг или ряд показателей, рассчитываемых на основе истории участия узла в проектах. При каждом выполненном задании оркестратор и валидаторы оценивают результат: успешное, корректное и своевременное выполнение повышает репутацию узла, а ошибки, срывы сроков или недостоверные результаты – понижают. Данный репутационный подход поощряет честное поведение, вознаграждая надежные узлы и применяя санкции к ненадёжным[7]. Это аналогично системе рейтингов на онлайн-платформах, где позитивный опыт повышает рейтинг пользователя, а негативный снижает.
Метрики репутации: Репутация может складываться из нескольких метрик. Например: процент успешно выполненных задач, среднее отклонение результатов от проверочных (если применимо), скорость отклика (насколько быстро узел берётся за задачу и выполняет её), время бесперебойной работы (uptime) и т.д. Эти составляющие могут комбинироваться в общий индекс. Платформа может публиковать базовые рейтинги узлов, стимулируя соревновательный эффект (как в проектах BOINC, где участникам присваиваются очки за вклад). Высокая репутация дает узлу преимущества: ему доверяют более крупные и оплачиваемые задания, он реже дублируется другими для проверки, может получать приоритетный доступ к новым проектам. Низкая репутация, наоборот, приведёт к тому, что узлу будут поручать только вспомогательные или дублирующие задачи, а то и вовсе временно отстранят от работы до восстановления доверия.
Вознаграждения: Чтобы привлечь больше участников и компенсировать затраты ресурсов, Newral предусматривает систему вознаграждений. Возможны разные модели вознаграждений: - Немонетарные (геймификация): Узлы получают очки и достижения. Например, выдается определённое количество кредитов за каждый час вычислений или за решённую задачу. На основе этих кредитов формируются рейтинги, награды (цифровые медали за определённый вклад, статус в сообществе). Такой подход применялся в добровольных распределённых вычислениях для мотивации энтузиастов. - Монетарные: В коммерческом или блокчейн-варианте платформы участники могут получать токены или другую криптовалюту за предоставленные ресурсы. Платформа может выступать как маркетплейс вычислений, где заказчик платит за выполнение задачи, а эта плата распределяется между узлами-исполнителями. В этом случае репутация тоже крайне важна – узлы с высокой репутацией могут получать бо́льшую долю вознаграждения или премиальные задания. Например, децентрализованная сеть Golem реализует оплату вычислительных ресурсов токенами, и при этом ввела механизм репутации, чтобы заказчики могли выбирать надёжных исполнителей[12][13]. - Вознаграждение репутацией: Кроме материальных и игровых наград, сами показатели репутации служат «наградой» для узла. То есть добросовестный узел автоматически получает больше доверия и, как следствие, больше возможностей заработать на будущих заданиях. Это формирует положительную обратную связь – доброе имя узла становится ценным активом.
Репутация для заказчиков: Хотя основной упор – на репутацию вычислительных узлов (поставщиков ресурсов), в системе можно учитывать репутацию и заявителей задач (клиентов). Это актуально в открытой экосистеме, где любые пользователи могут ставить задачи. Если заказчик consistently предоставляет валидные, корректно сформулированные задачи и своевременно осуществляет оплату (в случае платного режима), то ему доверяют, и его задания охотно берутся узлами. Если же заказчик замечен в злоупотреблениях (например, публикует некорректный код задач, который приводит к сбоям, или отказывается подтверждать результаты), его репутация падает, и узлы могут избегать его заданий. Такой двусторонний рейтинг, аналогично биржам фриланса, создает среду доверия с обеих сторон[14].
Механизмы реализации: С технической точки зрения, система репутации реализуется сервисом доверия (Dr. Mann#n) и базой данных. Каждое событие (завершение задачи, обнаруженная ошибка, проверка результата) генерирует событие для обновления репутации. Алгоритм обновления может быть простым (например, ±1 к рейтингу за успех/неуспех) или более сложным (взвешивание по сложности задачи, возрастающая стоимость ошибок для высокой репутации и т.п.). Репутация узлов хранится в БД и доступна оркестратору при планировании. Кроме того, она может быть публично видима через интерфейс (прозрачность рейтингов помогает участникам доверять системе).
Применение репутации в планировании: Оркестратор учитывает репутацию в нескольких аспектах: - Назначение задач: как упоминалось, задания с высокими требованиями по надёжности отправляются узлам с высокой репутацией. Наоборот, новые узлы (с низкой или нулевой репутацией) сначала получают простые или дублирующие задачи, где потенциальный ущерб от ошибки мал. - Объём параллельных задач: системе выгодно максимально использовать надёжные узлы. Поэтому узлу с хорошей репутацией может одновременно поручаться больше задач (в пределах его ресурсов), тогда как узлу с низкой репутацией нагрузку увеличивают постепенно, по мере подтверждения его стабильности. - Снижение контроля: репутация непосредственно влияет на степень проверки результатов (см. далее раздел о валидаторах). Узел с высоким рейтингом может освобождаться от принудительного дублирования результатов – платформа доверяет, но слегка проверяет. Например, в BOINC реализован адаптивный репликационный механизм, когда убедившись в надёжности хоста, сервер перестаёт дублировать каждый его результат[15]. Newral перенимает эту идеологию: «заслужил доверие – работай без лишнего контроля», что снижает избыточные вычисления и повышает общую эффективную мощность сети.
Награды и экономика сообщества: При внедрении монетарной награды важно учесть и экономическую модель – откуда берутся средства на оплату. Возможен вариант, когда проекты-заказчики оплачивают ресурсы (например, научные группы получают гранты и платят за вычисления), а узлы-исполнители получают вознаграждение пропорционально работе. При этом репутация влияет на доход: к примеру, задания от платёжеспособных заказчиков идут узлам с высокой репутацией, либо вводятся коэффициенты оплаты, зависящие от рейтинга узла (премия за надёжность). Такой подход обеспечивает справедливое распределение: проверенные узлы зарабатывают больше, новички сначала доказывают свою состоятельность делом.
Прозрачность и справедливость: Все правила начисления репутации и вознаграждений должны быть прозрачны и понятны участникам сообщества. Планируется публиковать политику репутации (какие действия как влияют на рейтинг) и возможно предоставить участникам возможность оспорить снижение репутации, если оно произошло, например, из-за системной ошибки, а не их вины. Это важно для поддержания доверия к самой платформе.
Система репутаций и вознаграждений превращает Newral не просто в вычислительный сервис, но в сообщество участников, объединённых общими целями и правилами. Она создаёт мотивированную экосистему: узлы стараются выполнить работу качественно, чтобы повысить свой статус и награды, а заказчики видят прозрачный рейтинг и могут выбирать исполнителей уверенно. В итоге, механизм репутации повышает надёжность всей системы (отсекая слабые звенья) и ускоряет вычисления (благодаря меньшей нужде в контроле за высокорейтинговыми узлами), что подтверждается опытом и других распределённых сетей[16].
Валидатор и проверка результатов
Распространённая проблема в распределённых вычислениях – как убедиться в корректности результатов, полученных от удалённых узлов, не пересчитывая заново каждое задание. В Newral реализуется многоуровневая схема верификации результатов, которая позволяет гарантировать правильность вычислений с минимальными избыточными повторениями. За координацию этих процессов отвечает компонент валидатора – это логическая роль (возможно, выполняемая оркестратором или отдельным сервисом), следящая за подтверждением результатов задач.
Необходимость проверки: Узлы могут возвращать некорректные результаты по разным причинам – сбой оборудования, ошибки в программном коде или намеренное жульничество. Без проверок система рискует принять ложные данные, что недопустимо. Наивный подход – считать каждое задание параллельно на двух и более узлах и сравнивать – резко снижает эффективность (например, удвоение вычислений в случае дублирования). Как отмечается, простая схема с двойным расчётом снижает эффективную производительность системы примерно вдвое[15]. Поэтому Newral применяет комбинацию методов, чтобы удостовериться в результатах, не требуя полной повторной обработки каждого задания.
Сертификаты корректности: Для некоторых типов задач возможна генерация специального артефакта – сертификата, доказывающего корректность решения. Например, если задача – тест на простоту числа, узел вместе с результатом («число простое») может предоставить сертификат простоты – набор данных, который быстро проверяется и убедительно свидетельствует о простоте (это может быть цепочка Люка или другие математические доказательства). Аналогично, для задач поиска (например, поиск пути в графе) узел может вернуть найденный путь как свидетельство. Платформа Newral поощряет включение механизма proof-of-computation в формулировку задач: разработчики проектов могут предусмотреть, чтобы решение сопровождалось проверяемыми метаданными. Валидатор, получив результат и такой сертификат, тратит существенно меньше ресурсов на верификацию, чем на полный перерасчёт. Если сертификат не подтверждён, результат отклоняется и задача отправляется на повторное вычисление (возможно, на другом узле).
Аудит и выборочная проверка: Даже когда задача не позволяет естественного сертификата, Newral внедряет подход выборочной (стохастической) проверки. Валидатор может случайным образом (либо по определённому алгоритму) выбирать небольшую часть задач для полного повторного вычисления на других узлах (или тем же узлом, если он повторит вычисление заново). Этот аудит служит для оценки надёжности узлов и выявления проблемных результатов. Например, из 1000 задач, выполненных узлом, платформа может перепроверить 1% заново. Если все проверенные совпали, высока вероятность, что и остальные корректны. Если же среди проверенных найдены расхождения, узел ставится под вопрос: возможно, придётся перепроверить гораздо большую долю или вообще заново выполнить работу этого узла. Такой подход существенно снижает накладные расходы – большая часть задач не дублируется, а лишь малая выборка, но при этом поддерживается контроль качества.
Репликация результатов: В критических случаях или для новых непроверенных узлов применяется репликация – отправка одного и того же задания на два и более узла параллельно. Результаты затем сравниваются. Если они совпали – велика уверенность в корректности, и обеим узлам засчитывается успех (или более доверенному засчитывается, а менее доверенный повышает рейтинг). Если результаты разошлись – включается разбирательство: возможно, отправка на третий узел (для каворума), и узел, давший расхожий ответ, теряет репутацию. Репликация – надёжный, но дорогой метод; в Newral он используется селективно. Как уже упоминалось, платформа стремится избегать постоянной двойной работы. Реализуется алгоритм, аналогичный адаптивной репликации BOINC: если узел доказал своё качество, его результаты принимаются без дублирования в большинстве случаев[15]. Репликация остаётся только для: - Первых задач нового участника (пока он не заслужил доверие). - Случайных проверок (в рамках аудита). - Особо важных вычислений, требующих абсолютной уверенности. - Разрешения конфликтов, когда поведение узла подозрительно.
Детерминизм и контроль выполнения: Валидатор также следит за тем, чтобы задачи выполнялись в детерминированных условиях. По возможности, клиентское ПО фиксирует версию приложения, исходные данные, параметры окружения и пр. Это важно, чтобы результаты разных узлов были сопоставимы. Например, если задача – моделирование со случайными числами, узел использует заданное зерно генератора, чтобы повторный прогон дал идентичный результат. Все такие параметры хранятся как часть метаданных задачи. Таким образом, когда валидатор поручает второму узлу повторить вычисление, он гарантирует идентичность входных условий. Это позволяет сравнение "бит-в-бит" для числовых результатов. При расхождении валидатор исключает факторы случайности или разнородности окружения – остаётся только заключить, что один из узлов ошибся или недобросовестен.
Кэширование результатов: Интересной особенностью Newral является накопление базы проверенных результатов. Если какая-то вычислительная задача (с определёнными входными данными) уже выполнялась ранее и результат был проверен/сертифицирован, платформа может повторно использовать этот результат, не запуская заново вычисление. Например, если узел вычислил значение некоторой функции для X и получен сертификат корректности, то при повторном запросе того же вычисления система просто выдаст сохранённый ответ (при условии, что задача идемпотентна и не зависят от времени). Такой механизм кэширования предотвращает повторные вычисления одинаковых задач в будущем. Конечно, он применим в случаях, когда задачи типовые и могут повторяться (например, проверка свойства числа, обработка конкретного файла и пр.). Сохранённые результаты помечаются метаданными (кто вычислил, когда, какой был сертификат/проверка) – это выступает своего рода сертификатом результата на будущее. Этот подход тоже увеличивает эффективность: вместо повторного расчёта достаточно проверить цифровую подпись или хэш готового результата и довериться ранее проведённой проверке.
Роль валидатора: Алгоритмически, функции валидатора могут выполняться самим оркестратором (как частью его логики) или выделенным сервисом. В распределённой реализации можно назначать некоторым узлам роль валидатора – то есть они не участвуют в первоначальных расчётах, а только перепроверяют чужие. Однако чаще экономически выгодно использовать любые узлы по мере их простоя для перепроверок. Оркестратор выступает координатором: он решает, какие задачи направить на проверку, кому поручить дублирование или аудит. При этом сам процесс проверки может рассматриваться как отдельная задача (например, задача типа "проверь результат задачи #N", тоже выполняемая узлом, но с повышенным приоритетом и логикой сравнения). После проверки валидатор (или оркестратор) подтверждает результат, и только после этого он считается окончательно выполненным и передается заказчику.
Сертификация результатов: Когда результат подтверждён (сертификатом, консенсусом или репутацией исполнителя), система может формировать цифровой сертификат результата – запись в БД, снабжённую контрольной суммой, метками времени, идентификаторами узлов-исполнителей и валидаторов. Этот сертификат может быть выдан заказчику как доказательство того, что вычисление выполнено корректно. В случае, если результаты Newral используются, к примеру, для научной публикации или критичного бизнес-решения, такой сертификат повышает доверие к полученным данным: он демонстрирует, что результат проверен внутренними механизмами платформы (и возможно, подписан приватным ключом платформы для неподдельности).
Благодаря многоступенчатой системе проверки, Newral достигает одновременно и высокой надёжности, и эффективности. Верификационные механизмы нацелены на минимизацию лишней работы: вместо постоянного дублирования используется умное комбинирование сертификатов, выборочных проверок и репутационного доверия. Известно, что адаптивные схемы могут свести потери производительности практически к нулю при достаточном уровне доверия к большинству хостов[15]. Именно это планируется реализовать: большинство узлов будут работать без дублей, а контроль сосредоточится на проблемных случаях. Таким образом, качество вычислений будет обеспечено, не снижая общей пропускной способности системы.
Хранение данных и переносимость
Для корректной работы распределённой платформы необходимо надёжное хранение как входных данных, так и результатов, а также состояние системы (задачи, прогресс, журналы событий). Архитектура Newral разделяет хранение на два основных уровня: реляционное хранилище (PostgreSQL) для оперативных данных и объектное хранилище для больших файлов и резервных копий.
PostgreSQL для структурированных данных: В центральной базе данных (СУБД PostgreSQL) хранится вся метаданная информация о платформе. Это включает: - Список зарегистрированных узлов (идентификаторы, характеристики, текущий статус, репутация). - Учетные записи пользователей/проектов, если платформа поддерживает многопользовательский режим. - Описание задач (их параметры, зависимости в DAG, какой проект/пользователь создал). - Состояние выполнения: какие задачи запущены, на каких узлах, прогресс, результаты, отметки валидации. - Журналы событий (важные события, ошибки, изменения статусов). - Репутационные данные, статистика по узлам. - Прочие служебные таблицы (например, токены аутентификации, настройки системы).
PostgreSQL выбран благодаря своей надёжности, поддержке транзакций и богатым возможностям запросов. Он обеспечивает целостность данных – например, гарантирует, что каждая задача переходит из состояния назначена в выполнена единожды, и не «теряется» при сбоях. Кроме того, СУБД позволяет выполнять сложные аналитические запросы для мониторинга (например, отбирать топ-узлы по репутации, или находить среднее время выполнения задач определённого типа).
Для повышения доступности базы применяются стандартные подходы: репликация БД на резервный сервер (hot standby), регулярные резервные копирования (ежедневные бэкапы снапшотов), а при больших нагрузках – шардирование или кластеризация (например, Patroni для PostgreSQL). Эти меры важны для сохранности состояния системы: даже если выйдет из строя основной сервер БД, резервный может взять на себя работу с минимальным простоем, а данные не будут потеряны.
Объектное хранилище для файлов: Распределённые вычисления могут оперировать большими объёмами входных данных (например, базы изображений, последовательности ДНК, графы) или генерировать крупные результаты (логи, матрицы, модели). Хранить такие бинарные блобы в реляционной БД неэффективно, поэтому Newral использует объектное хранилище, совместимое с S3 (например, MinIO, Ceph Object Storage или облачные S3-бакеты). Каждый файл (объект) хранится с уникальным ключом (например, хэш или ID задачи) и метаданными (размер, тип, дата загрузки). Объектное хранилище обеспечивает практически неограниченную масштабируемость по объёму: можно хранить терабайты и петабайты данных, добавляя узлы хранилища по мере роста потребностей. Кроме того, такие системы обычно сами поддерживают репликацию объектов или код с избыточностью, гарантируя сохранность (например, в MinIO объекты могут храниться с избыточностью на разных узлах, в Amazon S3 – трёхкратное резервирование на разных датацентрах).
Взаимодействие между БД и объектным хранилищем тесное: записи в БД содержат ссылки (ключи) на объекты файлов. Например, для задачи может храниться список ключей входных файлов и ключ результата. Метаданные в БД позволяют валидатору и оркестратору знать, какие файлы нужны узлу для старта задачи, и откуда их скачать. Когда узел получает задачу, он сначала через API объекта скачивает все нужные входные файлы (по ссылкам, выданным оркестратором), а по завершении загружает выходные файлы обратно. Клиентская часть обеспечивает проверку целостности при скачивании/отправке (сравнение хэшей) чтобы исключить повреждения сети[17].
Изолированность и переносимость состояния: Принцип, заложенный в архитектуре – отделение вычислений (узлы) от хранения (база + объекты). Узлы не содержат постоянного состояния: если узел внезапно отключился, его незавершённые задачи будут назначены другому узлу, и все необходимые данные для этого есть в центральном хранилище. Compute-узлы фактически статеслесс: они берут данные из объектного хранилища, и сохраняют результаты туда же. Это значит, что замена или добавление новых узлов не требует миграции данных – новые узлы просто подключаются к уже существующей базе и объектному бакету. Такой подход значительно упрощает масштабирование и восстановление. Например, как отмечено в современном дизайне облачных систем, при хранении состояния в S3/базе, вычислительные узлы легко возобновляют работу после сбоя – при запуске им достаточно узнать от БД, откуда загрузить своё состояние, и продолжить работу[3]. В Newral эта концепция реализуется для обеспечения высокой отказоустойчивости и гибкости.
Резервное копирование и миграция: Поскольку данные БД критичны, настраивается регулярное резервное копирование PostgreSQL (с определённой периодичностью – например, полный бэкап раз в сутки и журналы транзакций для точного восстановления). Объектное хранилище также резервируется – либо на уровне самого объекта (создание версий, снимков), либо копированием бакетов на удалённое хранилище. В случае какого-либо сбоя инфраструктуры, данные могут быть восстановлены из резервной копии с минимальными потерями.
Переносимость (портативность): Архитектура Newral не завязана жестко на конкретных технологиях конкретного вендора, что облегчает её перенос на разные среды. PostgreSQL – открытое ПО, его можно развернуть как локально, так и в облаке (например, AWS RDS, Azure Database и пр.). Объектное хранилище S3-совместимое: теоретически, можно мигрировать данные из локального MinIO в Amazon S3 или Google Cloud Storage, просто скопировав бакеты, поскольку интерфейс остается тем же. Контейнеризация микросервисов и использование Kafka также способствует облаконезависимости – всю платформу можно развернуть как on-premise (в своей инфраструктуре), так и в любом облаке, или даже распределить между несколькими центрами.
Если возникает необходимость переехать с одного датацентра в другой, процесс будет следующим: поднять новое хранилище (Postgres + объектное) в целевом месте, восстановить на нём последнюю резервную копию БД, перелить бакеты объектов (возможно, параллельно, так как объекты автономны), затем переключить узлы на новый адрес. Благодаря неизменности данных в S3 (они иммутабельны, обычно не меняются после записи), перенос может быть выполнен без сложной синхронизации – достаточно убедиться, что все новые результаты во время миграции пишутся уже в оба хранилища, либо на время миграции приостанавливать приём новых задач.
Локальные кэши узлов: Чтобы снизить сетевую нагрузку, узлы могут использовать локальный кэш для часто запрашиваемых данных. Например, если одна и та же библиотека или большой файл требуется многим задачам, узел, однажды скачав его из объекта, может сохранить на диске и использовать повторно (при условии валидности). Это ускорит запуск задач. Однако важно, что оригинал данных всё равно хранится централизованно, а кэш – лишь оптимизация.
Kafka и логи: Помимо основных данных задач, платформа генерирует большой поток событий (логи, метрики). Kafka выступает и как хранилище журнала событий на ограниченное время (например, хранение сообщений 7 дней). Для долгосрочного хранения логов может использоваться отдельный сервис (например, Elasticsearch/Graylog для логирования). Это уже детали реализации, но они дополняют картину хранения: все важные сведения протоколируются, и их можно проанализировать задним числом для отладки или аудита.
Подводя итог, подсистема хранения данных Newral обеспечивает надежность, масштабируемость и удобство управления данными. Разделение на БД и объектное хранилище позволяет оптимально работать с разными типами данных, а встроенные механизмы резервирования и переносимости гарантируют, что платформа сможет пережить сбои и адаптироваться к новым инфраструктурным условиям. Это фундамент, на котором строятся все вычислительные возможности Newral.
Шина событий Apache Kafka
Для координации взаимодействия микросервисов в Newral используется шина событий, основанная на Apache Kafka. Kafka – это высокопроизводительная распределённая система обмена сообщениями (коммит-лог), идеально подходящая для организации событийно-ориентированной архитектуры. В платформе Newral Kafka выполняет роль центрального коммуникационного посредника, через которого проходят практически все сообщения между компонентами.
Публикация и подписка: Компоненты системы взаимодействуют по модели publish/subscribe. Каждый тип события публикуется в определённый топик Kafka. Например: - Оркестратор публикует событие task_assigned (задача назначена узлу) в топик tasks с указанием ID задачи и адресата. - Узел, приняв задачу, публикует событие task_started в топик task_status с пометкой времени. - По завершении узел публикует task_completed или task_failed с результатом или ошибкой. - Валидатор (если отдельный) публикует task_verified или task_disputed после проверки результата. - Сервис репутации может публиковать reputation_changed и т.д.
Другие сервисы подписываются на интересующие их топики. Оркестратор подписан на топики с результатами и статусами, чтобы узнавать о завершении задач. Сервис репутации подписан на события завершения и проверок, чтобы обновлять рейтинги. Клиентский интерфейс может быть подписан на события прогресса, чтобы отображать пользователю, как выполняется его задача.
Такой подход обеспечивает слабую связность: отправитель события не знает, кто его получатель – он просто публикует в Kafka, а уже любой заинтересованный сервис читает сообщения[18]. Это означает, что легко добавить новый сервис (например, сервис аналитики, считающий статистику использования) – он просто начнёт слушать нужные топики, не требуя изменений у отправителей.
Высокая производительность и масштабируемость: Kafka славится способностью обрабатывать огромные объёмы сообщений с минимальной задержкой. Он достигает этого за счёт разделения на партиции и репликации. В контексте Newral это позволяет системе масштабироваться: при росте числа узлов и объёма событий можно увеличить число партиций топиков и Kafka-кластер (брокеры), так что обработка сообщений рассредоточится. Через Kafka могут проходить тысячи событий в секунду (например, при большом количестве мелких задач) – она справится благодаря горизонтальному масштабированию. Кроме того, Kafka хранит сообщения с репликацией на нескольких узлах, что гарантирует надёжность доставки: даже сбой брокера не приводит к потере событий[19].
Хранение и повторное чтение событий: В отличие от традиционных брокеров сообщений, Kafka сохраняет все сообщения в журнале на определённое время. Потребители не уничтожают сообщение при чтении, а лишь помечают свой офсет (позицию). Это позволяет: - При сбое потребителя (например, сервис репутации был перезапущен) – прочитать пропущенные события заново с последнего сохранённого офсета, ничего не потеряв. - Новому потребителю подключиться и прочесть историю событий (например, при отладке можно поднять временный сервис и «прокрутить» поток событий заново). - Масштабировать потребителей: можно создать группу нескольких экземпляров одного сервиса, они разделят между собой партиции топика и будут параллельно обрабатывать разные части потока.
Такая семантика очень удобна для Newral. Например, если оркестратор перезапускается, он может прочитать все task_completed во время своего даунтайма и обновить состояние задач. Или при увеличении числа узлов-валидаторов, они могут параллельно читать топик задач на проверку, разделив нагрузку.
Очередь задач: Kafka можно использовать не только для статусов, но и непосредственно как распределённую очередь задач. Один из вариантов реализации: оркестратор не назначает конкретному узлу, а публикует задачу в топик pending_tasks. Узлы-консумеры в группе читают этот топик и каждый берёт себе следующую задачу, если готов. Таким образом достигается автодисбалансировка: узлы сами разбирают очередь по мере готовности. Однако, чтобы учесть разную производительность, оркестратор всё же может направлять задачи более явным образом. На практике, комбинированный вариант – оркестратор публикует задачу с указанием требуемых ресурсов, а подписанные узлы сами решают брать её или нет, подходя по критериям. Тем не менее, чаще реализуют прямое назначение, а Kafka используют как гарант доставки и журнал.
Мониторинг через события: Kafka-шина служит общей «нервной системой» платформы, по которой текут данные о её состоянии. Это открывает возможность подключить системы мониторинга и аналитики. Например, можно настроить коннектор Kafka -> мониторинговая система, чтобы все события task_failed попадали на дашборд, сигнализируя о проблемах. Или события о производительности узлов (публикуемые агентами, см. раздел клиентской части) собираются для анализа нагрузки. Фактически, Kafka позволяет в реальном времени отслеживать работу Newral и реагировать на сбои быстрее, чем при опросе базы. Такой реактивный мониторинг повышает оперативность управления.
Пример потока событий: Рассмотрим цепочку: проект отправил новую задачу. Оркестратор публикует task_created (новая задача, ожидает назначения). Сервис планирования ловит его, решает на какой узел послать, публикует task_assigned (task_id, node_id). Узел (агент) получает это через подписку, начинает работать, публикует task_started (task_id, node_id). По завершении публикует task_completed (task_id, node_id, result_location). Валидатор подписан на task_completed, запускает проверку – если все ок, публикует task_verified (task_id). Оркестратор слушает task_verified и помечает задачу как успешно выполненную, уведомляет заказчика (например, генерирует событие task_result_ready). Одновременно, сервис репутации слушает task_verified и увеличивает рейтинг узла. Если же вместо task_verified пришёл task_disputed (task_id), значит были проблемы – оркестратор помечает задачу на повтор, а репутация узла снижается. Вся эта сложная логика реализуется асинхронно через Kafka, что делает систему более устойчивой к задержкам и отказам, чем если бы использовались прямые запросы.
Надёжность и порядок: Kafka гарантирует упорядоченность сообщений в пределах партиции. Если нужно строгий порядок для событий одной задачи, можно ключировать сообщения по task_id, чтобы все события конкретной задачи шли в одной партиции (Kafka гарантирует порядок для одного ключа). В Newral это можно задействовать, чтобы, например, события старта/окончания задачи не перепутались. Репликация Kafka на нескольких брокерах делает доставку сообщений надёжной даже при падении одного сервера – оставшиеся брокеры продолжают работу, а потребители переключаются на них автоматически. Такая fault-tolerance Kafka соответствует требованию высокой доступности платформы.
Подытоживая, использование Kafka предоставляет Newral масштабируемую, гибкую и надёжную коммуникационную инфраструктуру. Это позволяет микросервисам платформы взаимодействовать без жёсткой привязки друг к другу, в режиме реального времени обмениваясь событиями. Благодаря этому система легко расширяется новыми компонентами и выдерживает нагрузку по мере роста числа задач и узлов[20][21]. Kafka является промышленным стандартом для подобных задач, поэтому выбор её в стеке технологий Newral обеспечит проверенное временем качество и множество возможностей для дальнейшего развития (например, потоковую обработку данных и интеграцию с Big Data инструментами).
Клиентская часть: метрики, безопасность, ограничения
Клиентская часть платформы – это программное обеспечение, работающее на каждом вычислительном узле (агент). Именно от корректной работы клиентского агента зависит эффективность и безопасность выполнения задач на узлах-добровольцах. В дизайне Newral клиентскому приложению уделяется особое внимание: оно должно одновременно обеспечивать максимальную производительность вычислений, сбор нужной телеметрии, защиту хоста пользователя и уважение заданных пользователем ограничений.
Сбор метрик и мониторинг: Агент Newral на узле постоянно отслеживает ключевые метрики системы: загрузку CPU и GPU, использование оперативной памяти, объём сетевого трафика, дисковую активность. Эти данные служат двум целям. Во-первых, они передаются оркестратору (через события Kafka или прямые сообщения) – это позволяет оркестратору понимать текущую загрузку узла в реальном времени и принимать оптимальные решения по распределению задач. Например, если узел близок к перегрузке по памяти, оркестратор временно не будет слать ему новые задачи. Во-вторых, метрики могут использоваться для локального самоконтроля: клиент может приостанавливать выполнение задачи, если, например, температура CPU превысила критический порог (чтобы не повредить железо), либо если пользователь начал активно использовать компьютер (чтобы не мешать ему).
Клиентское приложение собирает статистику о каждой задаче: сколько времени она работает, сколько ресурсов потребляет. Эти данные публикуются как события (например, обновление прогресса). Оркестратор, получая такие события, может построить более точную модель длительности задачи (возможно, с помощью ИИ, предсказывая время завершения по ранним метрикам). Также метрики хранятся в БД для последующего анализа эффективности узлов и выявления узких мест. В интерфейсе платформы (например, веб-портале) пользователь-волонтёр сможет видеть графики: загрузка его узла, выполненные задачи, полученные награды и т.п., что создаёт эффект прозрачности и контроля.
Управление задачами на узле: Клиентская часть способна контролировать выполнение задач локально. Она запускает каждый полученный от оркестратора вычислительный процесс (например, исполняемый файл задачи) и следит за ним. Если системные условия требуют – агент может приостановить задачу, возобновить или прервать её. Это реализуется за счёт возможностей OS (приостановка потока, сигналы) или через взаимодействие с самим приложением задачи (если оно поддерживает checkpoint/restore). Как отмечено в документации BOINC, клиент обязан уметь приостанавливать, возобновлять и останавливать задачи, а также мониторить их потребление CPU и памяти[22]. Newral-клиент обеспечивает эти функции. Например, если пользователь на узле активировал настройку "не выполнять задачи, когда я работаю за компьютером", то при обнаружении пользовательской активности агент приостанавливает задачи (освобождая CPU) и возобновляет их, когда компьютер простаивает.
Изоляция и безопасность выполнения: Одно из важнейших требований – защита компьютера-участника от вредоносного кода, который потенциально может содержаться в задачах (особенно если задачи поступают от сторонних проектов). Клиент Newral запускает каждую задачу в изоляционном окружении. Возможны разные уровни: - Запуск приложения от имени низкопривилегированного пользователя ОС, в ограниченном режиме (без доступа к критичным системным ресурсам). - Использование контейнеризации (Docker) – задача запускается внутри Docker-контейнера, где явно определены доступные ресурсы и нет доступа к файловой системе хоста (кроме специально смонтированных каталогов для входных/выходных данных). Контейнеры хорошо подходят, если задачи требуют специфических сред (библиотек); образ контейнера задачи может поставляться проектом. - Использование виртуальных машин – для максимальной песочницы. BOINC, например, внедрил VirtualBox-решение: задача запускается внутри легковесной VM, изолированной от хоста[23]. Это обеспечивает почти полную защиту, ценой некоторой потери производительности. - Современные подходы, как WebAssembly sandbox или gVisor, тоже могут применяться для изоляции кода.
В MVP-версии, возможно, будет достаточно первого варианта (низкие привилегии + проверка подписи кода), но в перспективе Newral планирует поддерживать контейнеры для задач. То есть проект, загружая задачу, может предоставить Docker-образ, гарантируя нужную среду. Клиент при получении такой задачи запустит её через Docker-руntime. Изоляция важна не только для защиты узла, но и для воспроизводимости результатов – в контейнере у всех узлов одинаковое окружение, а значит, исключаются различия из-за разных версий библиотек или ОС.
Проверка подлинности приложения: Как уже упоминалось, клиент перед запуском проверяет хэш и цифровую подпись полученного исполняемого файла задачи[8]. Эти метаданные приходят от сервера (оркестратора) вместе с заданием. Если проверка не пройдена – задача отклоняется и сообщается об ошибке безопасности. Это предотвращает случай, когда злоумышленник перехватил соединение (при отсутствии TLS) и подменил приложение, или если кто-то пытается подсунуть узлу неавторизованное задание.
Ограничения и настройки пользователя: Платформа уважает желания владельца узла относительно того, как и когда использовать его ресурсы. Клиентское ПО предоставляет гибкие настройки ограничений, аналогичные существующим системам: - Ограничение CPU: пользователь может задать, какой процент CPU времени максимально может потреблять платформа. Например, 50% – тогда агент будет либо запускать не все ядра, либо делать паузы (дuty cycle)[24], чтобы средняя загрузка не превышала указанную. Это используется для снижения тепловыделения и шума, как опция CPU Throttle. - Время простоя: можно указать, что вычисления должны выполняться только когда компьютер не используется. Агент отслеживает периоды простоя (никаких движений мыши/клавиатуры) и выполняет задачи только в эти периоды. Или настроить "только с 12 ночи до 8 утра". - Ограничение по памяти и диску: пользователь может задать максимальный объём RAM, который могут занять задачи, и место на диске для хранения файлов проектов[25]. Агент тогда не будет принимать задачи, если они требуют больше памяти, и будет чистить/не кэшировать данные сверх лимита диска. - Ограничение по сети: если узел на тарифе с ограниченным трафиком, можно указать, сколько МБ/ГБ в день разрешается передавать. Агент будет регулировать скачивание/отправку файлов, укладываясь в лимит, и откладывать менее срочные передачи на следующий период, либо откажется от задач с большими данными. - Выбор проектов: если платформа хостит несколько разных проектов, пользователь может отметить, какие сферы его интересуют (например, математика, биомедицина) – агент будет предпочитать задачи тех проектов, которые соответствуют предпочтениям. (В BOINC есть механизм keyword preferences, позволяющий волонтёрам выбрать желаемые темы исследований[26][27].) - Количество параллельных задач: на многопроцессорном узле пользователь может ограничить, сколько задач одновременно можно запускать (например, не более N ядер нагружать, оставляя остальные для себя). - Работа от аккумулятора: типично, если компьютер работает на батарее (не подключен к сети), агент может приостанавливать задачи, чтобы не разряжать аккумулятор.
Все эти ограничения сохраняются либо локально в конфиге клиента, либо через веб-интерфейс профиля (как у BOINC, где prefs синхронизируются на все машины пользователя[28]). Клиент регулярно подтягивает обновления настроек и следует им.
Уважение к пользователю: Главный принцип – платформа не должна мешать владельцу узла. Поэтому клиент разработан так, чтобы работать тихо, на фоне. Низкий приоритет процессов задач (агент запускает задачи с пониженным приоритетом) гарантирует, что любые пользовательские приложения получат CPU вне очереди и не будут тормозить[29]. Для GPU задач тоже применяют методики, чтобы не мешать графическому интерфейсу (например, вычисления идут, только когда экран блокирован, и останавливаются при движении мыши).
Интерфейс пользователя на узле: Помимо невидимой работы, у клиента есть интерфейс (возможно, значок в трее или веб-страница локального хоста), где пользователь может увидеть: - Список текущих задач, их прогресс (% выполнения, время до конца). - Сводку: сколько ресурсов занято, температура (при наличии датчиков). - Статистику за всё время: сколько задач выполнено, какой эквивалент потраченного CPU-времени, сколько очков/токенов заработано. - Настройки, описанные выше, которые можно менять на лету. - Логи (например, сообщения об ошибках задач, если какие-то задачи упали).
Автообновление и безопасность клиента: Клиентское ПО также самообновляется до новых версий (с разрешения пользователя или автоматически, в зависимости от настроек). Это важно, чтобы быстро закрывать уязвимости. Обновления подписываются цифровой подписью платформы для безопасности.
Обратная связь от клиента: Узел может предоставлять и данные для улучшения системы. Например, агмент может анонимно отправлять профили производительности задач (сколько cache miss, загруженность диска), что поможет разработчикам задач оптимизировать код. Также узел может сигнализировать оркестратору о локальных проблемах – если, скажем, обнаружил сбойное состояние GPU, он может просить не слать ему GPU-задачи. В целом, агент – глаза и руки оркестратора на месте, поэтому он наделён некоторым интеллектом на периферии: принимает базовые решения мгновенно (приостановить, если перегрев), чтобы не ждать команды сервера, и информирует сервер для глобальной картины.
Итак, клиентская часть Newral сочетает функциональность управления (запуск/остановка задач, сбор результатов) с служебной функциональностью (мониторинг, безопасность, отчётность). Следуя лучшим практикам добровольных вычислений[30][31], агент обеспечивает, чтобы участие узла в платформе было максимально прозрачным, настраиваемым и безопасным для его владельца. Это повысит удовлетворённость и доверие участников, что напрямую влияет на успех всей распределённой сети.
Поддержка DAG-задач и проекты BPSW-Hunter и Кармайл
Поддержка задач-DAG: В отличие от традиционных систем добровольных вычислений, ориентированных главным образом на независимые параллельные задачи (embarrassingly parallel problems), Newral изначально спроектирована для поддержки сложных вычислительных конвейеров, представленных в виде DAG (Directed Acyclic Graph). Это означает, что платформа может выполнять не только разрозненные задания, но и целые workflows, где одни задачи зависят от результатов других.
Оркестратор Newral понимает структуру DAG-заявки: когда пользователь или проект загружает набор связанных задач, он описывает зависимости между ними (например, задача C начнётся только после завершения A и B, задача D зависит от C и т.д.). Оркестратор регистрирует все узлы графа в системе, присваивает им идентификаторы и отслеживает их состояния. Выполнение DAG происходит следующим образом: 1. Оркестратор находит задачи без входящих зависимостей (истоки графа) – их можно запускать сразу. 2. Когда узел завершыет выполнение задачи, оркестратор помечает её как завершённую и генерирует события, сигнализируя зависимым задачам об удовлетворении части их требований. 3. Когда для некоторой задачи выполнены все задачи, от которых она зависит, оркестратор переводит её в состояние готовности к запуску и ставит в очередь на исполнение. 4. Далее задача назначается узлу и выполняется, после чего процесс повторяется.
Такая схема делает возможным построение многоэтапных вычислительных процессов. К примеру, может быть трёхфазная обработка данных: сначала параллельно собираются или загружаются данные (этап 1), затем выполняются вычисления на этих данных (этап 2, зависимый от 1), затем агрегируются результаты (этап 3, зависящий от всех задач этапа 2). Оркестратор автоматически пройдет через эти фазы, не требуя ручного вмешательства.
Поддержка DAG облегчает реализацию сценариев машинного обучения и больших данных (например, обучение модели, затем применение её на разных наборах, затем суммарный анализ), а также научных расчетов, где одна программа генерирует промежуточные данные для другой. Платформа выступает аналогом систем типа Apache Airflow или Luigi, но с распределённым исполнением на внешних узлах. Можно сказать, Newral совмещает свойства менеджера потоков задач и среды распределённых вычислений.
Для эффективной работы с DAG важно оптимизировать промежуточное хранение. Объектное хранилище Newral служит местом, где задачи предыдущих этапов сохраняют результаты, доступные затем задачам последующих этапов. При этом применяется локализация: если возможно, задачи, потребляющие данные, выполняются на тех же узлах, где эти данные находятся (кешированы), чтобы снизить сетевые перегонки. Оркестратор может учитывать фактор data locality при планировании DAG-задач.
Проект BPSW-Hunter: Одним из пилотных применений платформы Newral является проект под кодовым названием BPSW-Hunter. Он посвящён исследованию математической проблемы, связанной с тестом простоты Baillie–PSW. Этот вероятностный тест (названный по фамилиям Байи, Померанца, Селфриджа, Вагстаффа) примечателен тем, что до сих пор неизвестно ни одного составного числа, которое бы он ошибочно определял как простое[32]. Иными словами, нет найдено ни одного псевдопростого числа Baillie–PSW (хотя строго доказано, что такие могут существовать). Авторы теста даже назначили вознаграждение за нахождение контрпримера – $30, а позже была объявлена награда $2000 за обобщённую форму теста[33]. Проект BPSW-Hunter ставит цель либо обнаружить такое исключение (составное число, проходящее тест), либо продвинуть границы гарантированной надёжности теста на много порядков.
Задача BPSW-Hunter хорошо подходит для распределённых вычислений, так как подразумевает проверку очень большого количества кандидатов – больших чисел на предмет прохождения ими теста. Эти проверки могут выполняться параллельно на множестве узлов Newral. Архитектура DAG для такого проекта, возможно, тривиальна (многие независимые задачи проверки чисел), но можно и усложнить: например, разбить поиск на диапазоны, где каждый узел проверяет свой диапазон чисел на псевдопростоту. Здесь DAG будет двухуровневый: узлы первого уровня генерируют кандидаты (или получают свой диапазон), узлы второго – проверяют их тестом BPSW и при нахождении подозрительного кандидата могут запустить дополнительную проверку или факторизацию, подтверждающую составность.
Роль Newral: Платформа предоставляет BPSW-Hunter необходимые ресурсы и надёжность проверки. Поскольку любые найденные потенциальные контрпримеры требуют особой проверки, система Newral гарантирует их повторную независимую проверку (через валидатор) и при возможности выдаст сертификат: например, если найдено число, якобы проходящее тест, валидатор должен подтвердить, что оно действительно составное (т.е. найти его делители) и что тест его ошибочно классифицировал. Это крайне ответственно, ведь такой результат – математическая сенсация. Благодаря механизму доверия, если узел заявил о находке, платформа автоматически направит эту задачу на несколько независимых узлов или специальных узлов-валидаторов, чтобы удостовериться, исключая тем самым ложные срабатывания из-за сбоев на одном узле.
Если BPSW-Hunter не находит контрпример, то он, тем не менее, расширяет границу применения теста. На 2023 год известно, что до 2^64 (~1.84e19) никаких псевдопростых BPSW нет[32]. Распределённые вычисления позволят возможно проверить числа вплоть до гораздо больших величин (например, 10^20, 10^21 и выше). Каждый узел Newral внесёт свой вклад, а оркестратор будет координировать, чтобы не было пересечений диапазонов и покрытие было полным. Репутационная система здесь тоже поможет: узлам с высокой репутацией можно доверить проверку более широких диапазонов без дублирования, а критические случаи всё равно проверяются повторно.
Проект Кармайл: Ещё один планируемый проект – Кармайл (Carmile), названный так по ассоциации с числами Кармайкла. Числа Кармайкла – это составные числа, которые обладают свойством проходить простейший тест простоты (тест Ферма) по всем основаниям[34]. Их бесконечно много, но они встречаются крайне редко[35][36]. Проект Кармайл, вероятно, связан с поиском новых больших чисел Кармайкла или исследованием их распределения. Это тоже вычислительно сложная задача: проверять большие диапазоны на наличие таких специфических составных чисел, либо генерировать кандидаты по критериям (например, используя критерий Корсельта для Кармайкла).
В контексте Newral, проект Carmile может быть организован как DAG, включающий: - Генерацию множества кандидатов (возможно, по параметрическим формулам) – первый этап. - Проверка каждого кандидата на критерий Кармайкла (проверить, является ли он составным, square-free, и удовлетворяет ли условию p-1 | n-1 для каждого простого делителя p) – параллельный второй этап. - Дополнительную валидацию найденных чисел (например, факторизация для убедительности).
Такая цепочка идеально подходит для распределения: генерацию можно распараллелить, проверки – тем более (каждый узел получает список кандидатов и проверяет их независимо). Если для факторизации требуется отдельный мощный ресурс, можно задействовать узлы с нужным ПО, или интегрировать систему с внешним решателем.
Ценность Newral: Для Carmile-проекта Newral обеспечивает тот же набор преимуществ: масштаб, надежность, проверяемость. Обнаружение нового огромного числа Кармайкла требует убедительных доказательств. Платформа, обнаружив кандидат, может автоматически: 1. Распределить его факторизацию по нескольким узлам (если число тысячи бит, факторизация – непростая задача, но распределенно возможная). 2. Проверить условия критерия Корсельта. 3. Собрать полный отчёт (сертификат), подтверждающий, что число действительно Кармайкла (привести разложение на простые, показать, что для каждого простого p делящего n, p-1 делит n-1).
Такой отчёт – ценная научная информация. Благодаря Newral, он будет получен быстро и без ошибок, так как все шаги контролируются (каждая факторизация или проверка может дублироваться для надёжности, узлы с сомнительной репутацией не будут привлекаться к завершающим критическим проверкам).
Workflow-различия: Проекты BPSW-Hunter и Carmile иллюстрируют два типа типовых задач платформы: - Массовый параллелизм с независимыми задачами: как в BPSW-Hunter (каждое число проверяется отдельно). Здесь основной вызов – раздать огромный пул простых заданий тысячам узлов и собрать результаты. - Вычислительные цепочки с зависимостями: как в Carmile (генерация -> проверка -> факторизация), где Newral выступает как координатор сложного алгоритма, разбитого на этапы.
Платформа спроектирована гибко, чтобы обслуживать оба сценария. DAG-движок позволяет описать практически любую комбинацию таких этапов.
Интеграция и поддержка: Для успешного выполнения подобных проектов важна поддержка специфичных алгоритмов. Newral планирует предлагать библиотеки/SDK для разработки задач, чтобы математики могли легко подключить свои алгоритмы. Например, может быть предоставлен шаблон проекта для поиска чисел Кармайкла, куда ученый встраивает свою генерацию кандидатов, а платформа берёт на себя распараллеливание и проверку. Аналогично, для BPSW – библиотека длинной арифметики и реализация теста BPSW будут доступны и оптимизированы под разные архитектуры (x86, ARM, GPU?), так что узлы будут эффективно их использовать.
Распределённые проекты сообщества: Структура Newral позволяет одновременно работать над несколькими проектами (подобно BOINC, где множество проектов). BPSW-Hunter и Carmile могут запускаться параллельно, и узлы могут распределять свой ресурс между ними согласно предпочтениям или указаниям оркестратора (возможно, пропорционально важности или вознаграждению проектов). Благодаря Kafka, события от разных проектов разграничиваются, но платформа остаётся универсальной.
В конечном счёте, примеры BPSW-Hunter и Carmile демонстрируют, что Newral нацелен не только на стандартные задачи типа "рендеринг видео" или "обработка изображений", но и на фундаментальные научные вычисления, требующие как большого количества ресурсов, так и высочайшей точности. Платформа станет удобным инструментом для исследователей: она снимет с них заботы о распределении заданий и проверке результатов, предоставив готовую надёжную инфраструктуру. А сообщество волонтёров (или оплачиваемых участников) сможет участвовать в решении этих задач, будучи уверенным, что их вклад не пропадёт даром и что каждый результат будет проверен и доведён до достоверного итога.
Стек технологий
Архитектура Newral реализована с использованием современных технологий, обеспечивающих масштабируемость, производительность и удобство разработки. Ниже перечислены основные компоненты технологического стека и их назначение:
•	Языки программирования: Платформа разработана полиглотно. Критически важные компоненты, требующие высокой производительности и низкого уровня, реализованы на C++/Rust (например, вычислительные библиотеки длинной арифметики для проектов типа BPSW-Hunter). В то же время, оркестратор и вспомогательные сервисы написаны на высокоуровневых языках – Python (с использованием фреймворка FastAPI для API и библиотек машинного обучения для AI-режимов) и Go для микросервисов, где важна конкурентность и простота развертывания. Возможен и выбор Java/Scala (экосистема Kafka хорошо поддерживается, а для сложной логики оркестрации с AI может применяться Scala/Akka). Таким образом, используется подход "лучший инструмент для задачи" – микросервисная архитектура позволяет каждому сервису быть написанным на оптимальном языке[37], взаимодействуя через стандартизированные интерфейсы.
•	API и коммуникации: Взаимодействие между сервисами построено преимущественно на обмене сообщениями через Apache Kafka, как описано выше. Кроме того, реализованы REST API (или GraphQL) эндпойнты для внешних запросов – например, для отправки новой задачи или запроса статуса. Внутри кластера возможно также использование gRPC для синхронных вызовов между некоторыми сервисами (например, между оркестратором и валидатором, если потребуется). Для API-шлюза используется веб-фреймворк (FastAPI/Python или Spring Boot/Java), обеспечивающий аутентификацию и агрегирование данных.
•	СУБД: Реляционная база PostgreSQL – основной выбор для хранений метаданных. Используются возможности PostgreSQL по хранению JSONB (для гибких структур, например, DAG-описаний), функции и процедуры для сложной логики (возможно, подсчёт агрегатов репутации). БД масштабируется вертикально (на мощном сервере) и горизонтально (читатели-реплики для распределения нагрузки на чтение). Для некоторых высоконагруженных кейсов (например, быстрый поиск по логам) может дополнительно применяться NoSQL хранилище или поисковый движок (ElasticSearch) – например, для анализа миллионов событий выполнения.
•	Хранилище объектов: Развернут MinIO – высокопроизводительное S3-совместимое объектное хранилище, работающее в кластере из нескольких узлов. Он хранит файлы задач, наборы данных, результаты. Преимущество MinIO – совместимость с S3 API и простое горизонтальное масштабирование. В зависимости от инфраструктуры, это может быть заменено на нативное облачное S3-хранилище (AWS S3, Google Cloud Storage) без изменений кода платформы.
•	Docker и оркестрация контейнеров: Все компоненты упакованы в Docker-контейнеры, чтобы обеспечить единообразие окружения и лёгкий деплой. Для управления контейнерами используется Kubernetes (K8s) – он занимается развертыванием микросервисов, их скейлингом и восстановлением при падениях[6]. K8s также облегчит перенос системы между разными средами (локальный кластер, облако). На стороне узлов, Docker используется для контейнеризации задач (если проект предоставляет Docker-образ, агент его скачает и запустит).
•	Kafka и Zookeeper: Kafka-кластер состоит из нескольких брокеров для надёжности. Если версия Kafka требует Apache Zookeeper, он тоже развёрнут для хранения метаданных брокеров (в новых версиях Kafka можно работать и без Zookeeper). Конфигурация Kafka настроена на высокую пропускную способность: несколько партиций для ключевых топиков (tasks, results, metrics и др.), репликация партиций (фактор 3). Используется Kafka Streams или ksqlDB для некоторых онлайн-обработок – например, подсчёта скользящих метрик или фильтрации потоков событий (это рассматривается для будущих версий).
•	Компоненты AI: Для реализации функций AI-оркестрации интегрированы библиотеки машинного обучения. Модуль планирования может использовать Python (NumPy/Pandas, scikit-learn) для анализов исторических данных, либо более продвинутые фреймворки – например, PyTorch для обучения модели, предсказывающей время выполнения задач на разных узлах. Возможно применение Reinforcement Learning (библиотеки типа TensorForce или OpenAI Gym) для обучения оркестратора в режиме симуляций. Эти компоненты работают оффлайн (обучение на исторических логах) и онлайн (выдача рекомендаций в Advisory-режиме). Для изоляции AI-логики может быть выделен микросервис SchedulerAI, который по запросам оркестратора предлагает решения (например, куда назначить задачу). В планах – эксперименты с графовыми нейронными сетями для моделирования всей сети узлов, либо применение существующих решений AI for IT Ops.
•	Web-интерфейс и инструменты пользователя: Интерфейс платформы (для администраторов проектов и возможно для волонтёров) реализован как веб-приложение. Фронтенд на React/Vue.js обеспечивает удобный дашборд: список проектов, статистика узлов, графики выполнения. Backend для этого интерфейса – тот же API-шлюз. ДляVolunteers можно также сделать мобильное приложение (на React Native, к примеру) для отслеживания прогресса. Однако это в перспективе.
•	Мониторинг и логирование: Для наблюдения за работой самой платформы (на уровне DevOps) развёрнут стек мониторинга: Prometheus собирает метрики со всех сервисов (загрузка CPU микросервисов, lag потребителей Kafka, состояние БД). Grafana используется для визуализации – например, дашборд с количеством активных узлов, выполненных задач, средним временем проверки и т.д. Логи сервисов централизуются с помощью ELK-stack (Filebeat собирает, Elastic хранит, Kibana анализирует). Это помогает команде разработки оперативно выявлять проблемы в продакшене.
•	Безопасность: Используются стандартные компоненты: Keycloak или аналог для единого управления аутентификацией и авторизацией через OAuth2/OpenID Connect. Он интегрирован с API-шлюзом, обеспечивая выдачу JWT-токенов для пользователей и сервисных аккаунтов. Для шифрования трафика – CertManager (в случае K8s) автоматизирует получение TLS-сертификатов (например, Let’s Encrypt) для внешних интерфейсов. Хранение секретов (пароли БД, ключи подписей) – через Kubernetes Secrets либо внешнее хранилище секретов (HashiCorp Vault), с ограниченным доступом.
•	Разработка и CI/CD: Проект использует системы контроля версий (Git) и настроен CI/CD pipeline (например, GitHub Actions, GitLab CI или Jenkins) для автоматической сборки образов и развертывания на кластер. Тестирование включает модульные тесты для логики оркестрации, имитационные тесты (симуляция сети узлов) и нагрузочные испытания.
Данный стек обеспечивает баланс между производительностью (низкоуровневые части на эффективных языках, Kafka для быстрого обмена, С/С++ libs для математики) и гибкостью (Python/JavaScript для высокоуровневой логики и интерфейсов, микросервисы для изоляции). Опора на открытые стандарты (PostgreSQL, Docker, S3, OAuth2 и пр.) гарантирует, что система не зависит от проприетарных решений и легко интегрируется с существующей ИТ-инфраструктурой. По мере развития платформы стек может расширяться – например, добавить поддержу GPU-вычислений через Nvidia CUDA (поддержка GPU-задач на узлах), что может потребовать специфичных библиотек (CUDA Toolkit, NCCL для распределённого обучения). Однако уже на этапе MVP выбранный технологический стек даёт прочную основу для реализации всех задуманных функций.
Дорожная карта MVP
Разработка платформы Newral разбита на этапы с прицелом на достижение MVP (Минимально жизнеспособного продукта) – версии, демонстрирующей ключевые возможности системы в действии. Далее представлены основные этапы roadmap и вехи развития:
1.	Этап 0 – Исследование и проектирование (Q1 2025): На этом этапе проводился анализ требований, сравнительный обзор существующих решений (BOINC, Golem, etc.), определение архитектуры. Была сформирована общая концепция микросервисной структуры, выбраны технологии (Kafka, PostgreSQL и др.), заложены принципы доверия и безопасности. Результатом этапа стали архитектурные схемы и прототипы отдельных компонентов (например, пробный запуск Kafka-соединения между имитацией оркестратора и узла).
2.	Этап 1 – Базовая инфраструктура и оркестратор (Q2 2025): Реализация ядра системы:
3.	Развёртывание основных сервисов: оркестратор, база данных, Kafka, API-шлюз.
4.	Написание простейшего планировщика в оркестраторе (режим AI Off) – задачи ставятся в очередь и распределяются round-robin по подключённым узлам.
5.	Разработка клиентского приложения-агента для узлов: соединение с оркестратором (через websocket или gRPC), получение задач, выполнение заглушки задачи (например, расчёт фиктивной функции) и возвращение результата.
6.	Реализация хранения файлов: поднят MinIO, интегрирован с оркестратором и клиентом (узел умеет скачивать входные данные задачи и загружать результаты).
7.	Обеспечение базовой безопасности: шифрование соединений, простая аутентификация узлов (предварительно сгенерированные токены).
8.	Демонстрация концевого-to-концевого сценария: запуск задачи (например, вычислить сумму большого массива) через API, оркестратор раздаёт задачу узлу, узел выполняет и возвращает, результат виден через API.
9.	На этом этапе система уже работает как распределённая вычислительная среда без расширенных функций доверия. MVP минимально состоится, если удастся запустить хотя бы один реальный вычислительный проект (напр. небольшой поиск простых чисел) с несколькими узлами.
10.	Этап 2 – Валидация результатов и надёжность (Q3 2025): Добавление механизма проверки:
11.	Разработка компонента валидатора. В оркестратор внедряются функции дублирования задач: по настройкам проекта или при низкой репутации узлов задачи отправляются на N узлов. Оркестратор сравнивает результаты.
12.	Внедрение адаптивной репликации: ведётся список узлов и их надежности; для надёжных узлов оркестратор снижает коэффициент репликации[15].
13.	Реализация сертификатов на примере одной задачи: для мат. задач (простой тест) агент формирует сертификат (например, список делителей для доказательства составности числа). Валидатор проверяет сертификат вместо полного пересчёта.
14.	Обработка расхождений: если пришли разные результаты, оркестратор помечает задачу как спорную и автоматически запускает её повторно на другом узле (или нескольких) для выяснения истины.
15.	Логика штрафов репутации: узлу, давшему неверный результат, снижается внутренний рейтинг.
16.	Проведение тестов: намеренно модифицированный клиент (шаловливый) запускается, возвращает ошибочные данные – проверяется, что система это выявляет и изолирует такой узел.
17.	К этому времени платформа достигает состояния, когда может гарантировать корректность результатов, хоть и с некоторыми издержками (репликация).
18.	Этап 3 – Система доверия и репутации (Q4 2025): Формирование социального слоя:
19.	Запуск сервиса репутации Dr. Mann#n. Определение метрик репутации и правил обновления (возможно, хранение в БД таблицы node_reputation с полями: успешно выполнено, провалено, подозрительные случаи и итоговый скор).
20.	Интеграция репутации в оркестратор: планировщик учитывает рейтинг при выборе узла (например, сортирует доступные узлы по убыванию рейтинга для важной задачи).
21.	Интерфейсы: отображение репутации узлов в админ-консоли, возможно, уведомление узла о его рейтинге через клиент.
22.	Реализация вознаграждений: для MVP может быть достаточно начисления "очков кредита" за каждую успешную задачу. Т.е. система ведёт счетчик очков для узлов (и пользователей, если есть учетные записи). Это покажет концепцию наград.
23.	Тестирование на сценариях: новый узел без репутации получает простые задачи, повышает рейтинг, потом ему дают большую задачу – убедиться, что логика работает. Попытаться "читерить" – смоделировать узел, который делает лишь вид работы, – убедиться, что он не заработает высокую репутацию без реальных результатов.
24.	Этап 4 – Поддержка DAG и сложных задач (Q1 2026): Расширение диспетчера задач:
25.	Внедрение возможности загрузки в систему задач с зависимостями. Определение формата (например, JSON или специальный DSL для описания DAG).
26.	Модификация оркестратора: хранить граф, счётчики выполненных зависимостей, выпускать задачи, когда они готовы. Использование событий Kafka для асинхронного срабатывания (узел завершил задачу -> событие -> оркестратор считает зависимости).
27.	Реализация хранения промежуточных результатов и передачи между задачами: один узел завершил – загрузил в хранилище, другой узел по ID задачи может скачать эти данные.
28.	Тест: запустить простой DAG: задача А и B параллельно (генерируют данные), задача C зависит от A и B (агрегирует). Проверить, что C стартует только после A и B и получает от них нужные данные.
29.	Оптимизация: добавить в планировщик учет локальности данных (на MVP уровне можно не делать сложно, но хотя бы предусмотреть).
30.	С помощью DAG-функциональности развернуть демонстрационный проект Carmile – например, поиск чисел Кармайкла до некоторого диапазона, с фазами "генерация кандидатов" -> "проверка". Это послужит доказательством работоспособности сложных workflow.
31.	Этап 5 – AI-оркестратор (Q2 2026): Включение интеллектуальных режимов:
32.	Сбор достаточного объема телеметрии с предыдущих этапов (логи выполнения, метрики узлов). На их основе начинается разработка ML-моделей.
33.	Advisory-режим: тренируется простая модель (например, регрессия или дерево решений), предсказывающая время выполнения задачи по ее параметрам и характеристикам узла. Эта модель встраивается в оркестратор: при назначении задачи рассчитывает на каких узлах она быстрее/надежнее выполнится, и предлагает вариант. Оркестратор может или автоматически следовать или показывать оператору (в режиме совета).
34.	Partial-режим: реализуется автоматизация конкретных аспектов – например, авто-реакция на сбой узла. Сейчас, если узел отключился, задачи на нём зависают до таймаута. В Partial-режиме оркестратор с ИИ может быстрее это обнаружить (на основе аномалий метрик) и перекинуть задачи, не дожидаясь стандартного таймаута. Другой пример – динамическое масштабирование: если очередь задач растет, ИИ-агент может рекомендовать привлечь дополнительные узлы (в облачной версии – автоматически запустить новые VM с агентами, в волонтерской – разослать уведомления спящим узлам).
35.	Full-режим: пока включать не планируется на MVP, но закладываются механизмы. Например, orchestrator получает флаг, что может самостоятельно менять приоритеты задач на основе своих прогнозов (что критично успеть раньше и т.п.). Для MVP full AI будет в экспериментальном режиме, работающем на тестовом стенде.
36.	Этап 6 – Шлифовка, безопасность и запуск MVP (Q3 2026): Финальные штрихи перед публичным запуском:
37.	Проведение нагрузочного тестирования: допустим, имитировать 1000 узлов, 10000 задач – проверить стабильность Kafka, БД, оркестратора. Оптимизировать узкие места (напр., увеличить партиции Kafka, настроить connection pool к БД).
38.	Улучшение безопасности: внедрить полноценную систему авторизации (пользовательские роли: админ, проект-менеджер, волонтер). Убедиться, что узел не может запросить чужие данные (проверки токенов на API).
39.	User Experience: доработать инсталлятор клиентского приложения (в идеале – один клик установка на Windows/Linux), понятный интерфейс настроек.
40.	Обновить документацию: описание API для проектов, гайд для участников по установке клиента, правила сообщества относительно репутации и вознаграждений.
41.	Запуск пилотного проекта на платформе – например, упомянутого BPSW-Hunter. Небольшая группа узлов (скажем, 50 машин) подключается, выполняет реальную работу. Сбор отзывов, наблюдение за системой в боевых условиях.
42.	Исправление выявленных багов, отладка производительности в реальной сети (например, может всплыть, что при потере связи с Kafka агент падает – это нужно обработать, и т.п.).
После успешного прохождения этих шагов, MVP версии Newral будет готова. Она включает: - микросервисную платформу с оркестратором, - безопасными коммуникациями, - механизмом доверия и репутаций, - проверкой результатов без тотальной репликации, - поддержкой DAG, - базовыми AI-функциями советника, - удобным клиентским ПО для участников.
Далее дорожно карту можно продлить в пост-MVP: - Масштабирование сети (поддержка десятков тысяч узлов, оптимизация под высокие задержки в интернете), - Полный AI-режим (добавление self-learning алгоритмов, возможно, нейросетевые планировщики), - Монетизация (внедрение крипто-токена или интеграция с платежными системами для реальных вознаграждений), - Сообщество и геймификация (публичные рейтинги, соревнования между командами участников, как в BOINC), - Новые проекты (расширение списка проектов, интеграция с научными организациями).
Однако эти шаги выходят за рамки MVP. Представленный план рассчитан на то, чтобы к концу указанного периода платформа продемонстрировала свою работоспособность на практике и привлекла первых пользователей. Успешное MVP станет основой для дальнейшего развития Newral в полномасштабную распределённую вычислительную экосистему нового поколения.[15][38]

English version
________________________________________
Newral: Next-Generation Distributed Computing Platform Architecture
Project Goals and Overview
Newral is a next-generation distributed computing platform designed to harness the combined power of many participant nodes for large-scale computational tasks. The project’s goal is to provide a flexible, secure, and intelligent infrastructure where diverse computing resources (from personal computers to servers) can collaborate to solve complex problems. Newral aims to overcome limitations of previous distributed computing systems by incorporating modern architectural principles, such as microservices and AI-driven orchestration, along with robust trust and verification mechanisms. In essence, Newral will enable secure and reliable distributed operations, ensuring that computations are performed correctly even in a decentralized environment[1].
Key objectives of Newral include: - Scalability: Efficiently utilize thousands of nodes in parallel, scaling out to handle tasks requiring massive aggregate CPU/GPU time. - Intelligent Orchestration: Use an AI-powered orchestrator to optimize task scheduling, resource allocation, and load balancing in real time, adapting to changing conditions. - Trust and Security: Establish a framework of trust among nodes (even if they are owned by different, potentially anonymous individuals) through reputation scores, verified results, and secure node authentication. Protect both the network and the participant nodes from malicious behavior. - Result Verification: Implement robust result validation techniques so that task outputs are correct without needing to redundantly recompute every task. This involves validators, cryptographic certificates of correctness, audits, and selective replication, as discussed later. - Support for Complex Workflows: Unlike traditional volunteer computing focusing mostly on independent tasks, Newral will natively support tasks with dependencies (DAGs – Directed Acyclic Graphs of tasks). This broadens the range of applications to multi-stage workflows in scientific computing, data processing pipelines, AI model training, etc. - Participant Incentivization: Create a fair and motivating system of reputation and rewards for participant nodes. Reliable contributors earn higher reputation and possibly rewards (points or tokens), encouraging honest participation and sustained contributions[7]. - Modularity and Maintainability: Architect the system as a set of independent microservices with clear APIs. This ensures each component can be developed, deployed, and scaled independently, making the platform more resilient and easier to extend.
In summary, Newral is envisioned as a distributed computing ecosystem where computing tasks can be executed efficiently at scale and with high assurance of integrity. It is a platform equally suited for volunteer scientific projects (like searching for mathematical objects or analyzing biomedical data) and for enterprise or academic use cases that require distributed computation with strong reliability guarantees. By blending microservice cloud architecture with the crowd-sourced computing model (and adding a layer of AI guidance), Newral represents a novel approach to distributed computation – one that is intelligent, secure, and community-driven.
Platform Architecture
Newral’s architecture is built on a microservices model, dividing the platform into discrete services that work together. This design choice improves modularity and scalability: each service can be scaled and updated independently to meet demand[2]. The major components of Newral are:
•	Task Orchestrator (AI Orchestrator): The orchestrator is the central “brain” of Newral. It is responsible for accepting new computational tasks, breaking them down into subtasks if necessary, and assigning these tasks to available nodes. It also tracks task execution and handles dependencies between tasks. Uniquely, the orchestrator in Newral is AI-augmented (hence AI Orchestrator): it can operate in various modes of autonomy (AI Off, Advisory, Partial, Full – detailed in the next section) to optimize scheduling and resource allocation. The orchestrator runs as a microservice, often replicated for high availability. In an event-driven fashion, it responds to node availability events, task completions, and other triggers to continuously keep the distributed system busy and balanced.
•	Worker Nodes (Agents): These are the machines (computers, servers, or even mobile devices) contributed to the platform that actually perform the computations. Each node runs a lightweight client agent that communicates with the orchestrator, receives tasks, executes the computational jobs, and returns results. Nodes can be heterogeneous in hardware and OS; the platform abstracts this via the agent. Newral’s architecture is designed to accommodate a wide variety of node capabilities – from CPU-only nodes to GPU-accelerated ones – and the orchestrator will match tasks to nodes that meet the requirements. The platform treats each node somewhat like a microservice instance that can join or leave. Nodes are stateless in terms of not holding permanent critical data; they fetch input data when needed and return outputs, relying on central storage (detailed later).
•	Trust & Security Service (Dr. Mann#n System): To manage security and trust, Newral includes a component informally named Dr. Mann#n. This system is in charge of maintaining node trust levels, evaluating reputation scores, and orchestrating result validations. It can be thought of as the platform’s immune system, identifying malicious or malfunctioning nodes and ensuring they do not compromise results. The Dr. Mann#n system integrates with the orchestrator: before assigning a task, orchestrator checks the trust level of candidate nodes; after task completion, it updates reputation and may trigger result audits. This service also handles node authentication and certification, ensuring that only authorized nodes with known identities can participate and that all communications and code deliveries are secure (e.g., tasks are code-signed).
•	Distributed Storage: Newral separates storage into two subsystems for efficiency:
•	A Relational Database (PostgreSQL) stores structured metadata: information about tasks (definitions, status, dependencies), node information and reputation, user/project accounts, etc. PostgreSQL provides strong consistency for these critical records, supporting complex queries and updates within transactions.
•	Object Storage (S3-compatible) is used for large binary data – input datasets, intermediate files, final outputs. Each object is typically referenced by a key (like a content hash or task ID) and stored durably with replication. Object storage scales to very large sizes and bandwidths, essential for data-intensive tasks. By using object storage for bulk data and PostgreSQL for metadata, Newral achieves a balance: metadata operations are quick and consistent, while big file transfers happen outside the database to avoid bottlenecks[3]. This design also means compute nodes do not need to retain big files after use; they can always re-fetch from the central store if needed, making nodes easier to replace or recover.
The combination of Postgres + object store makes compute nodes effectively stateless from a system perspective. If a node fails mid-task, another node can be given the same task and it will retrieve the input data from storage and resume processing (perhaps from the start or from the last checkpoint). The failed node’s partial results, if any, are not needed because tasks will be re-run or were checkpointed to storage. This stateless design improves fault tolerance: node crashes do not lose data, and tasks can be recovered or reassigned seamlessly[3].
•	Event Bus (Apache Kafka): Newral uses Apache Kafka as a message bus and event streaming platform to decouple services and enable high-throughput communication. All significant events in the system – such as “task assigned to node X,” “node Y completed task,” “node Z joined/left,” “reputation updated” – are published as messages to Kafka topics. Microservices subscribe to relevant topics: for instance, the orchestrator subscribes to task result events, the reputation service subscribes to task outcome events, etc. This decoupled publish/subscribe model allows the system to be loosely coupled and scalable, as each service can process events at its own pace and new services can tap into the event stream without modifying existing ones[4]. Kafka also acts as a durable system log, storing events for a certain retention period. This means if a service goes down and comes back up, it can replay missed events from Kafka to catch up, ensuring reliability. Additionally, Kafka’s high throughput and low latency support the platform’s need to handle potentially thousands of events per second without slowing down the decision loop[39].
•	API Gateway and User Interface: External users (project owners, administrators, or possibly volunteers monitoring their nodes) interact with Newral through a set of APIs and a web-based UI. The API Gateway (a microservice) provides a unified REST/GraphQL API to submit new tasks or workflows, query the status of tasks, and manage projects and nodes. It also handles user authentication and authorization. On top of the APIs, a Web Dashboard can be provided for human users, showing real-time metrics, task progress, and historical statistics in a friendly interface. This component is not critical to compute operations but is important for usability and transparency. It connects to the back-end services (like orchestrator and trust service) often by querying the Postgres database or via specialized API calls.
•	Additional Supporting Services: Other microservices in the ecosystem might include:
•	Monitoring/Telemetry Service: Collects system metrics and logs for analysis (could use existing tools integrated with Kafka).
•	Notification Service: Sends out alerts or notifications (email, etc.) based on certain events (e.g., a project’s tasks are completed, or a node’s reputation drops).
•	Scheduler/Autoscaler: In a cloud deployment, possibly controls scaling of compute resources (spinning up more VMs or containers for more compute power) in response to demand, though in volunteer scenarios this might not apply directly.
All these components interact primarily via asynchronous messages (and some synchronous control APIs where needed), forming an event-driven microservice architecture. The use of microservices yields several benefits: - Fault isolation: If one component fails (say the reputation service), other parts (task execution) can continue functioning; the system degrades gracefully instead of failing completely[40][2]. - Independent scaling: Services that handle a lot of load (e.g., the orchestrator or Kafka itself under heavy events) can be scaled out (replicated) without scaling the entire system monolithically[2]. - Polyglot implementation: Each service can be implemented in the language/technology best suited for it, as long as it speaks the common protocols (HTTP, Kafka, etc.)[37]. For example, heavy data analysis logic could be in Python, whereas high-performance networking might be in Go or Rust. - Ease of updates: New features can be added by introducing new services or updating existing ones in isolation, reducing risk and deployment downtime.
In deployment, these microservices are containerized (e.g., with Docker) and orchestrated by a container orchestration platform like Kubernetes, which handles scheduling them on cluster nodes, restarting failed instances, and scaling as needed[6]. Each component’s configuration (database connection strings, Kafka brokers, etc.) can be managed through config maps or environment variables for flexibility.
Node Roles and Heterogeneity: All compute nodes in Newral are fundamentally similar in function (execute tasks), but they can have different roles or capabilities: - General Worker: The standard role; executes any assigned tasks. - Validator Node: In some cases, certain nodes might be designated or temporarily used as validators to recompute tasks for verification purposes. However, more typically, any node can act as a validator when assigned a validation task. - Specialized Nodes: If tasks require specialized hardware (GPUs, high-memory machines, FPGA, etc.), the orchestrator categorizes nodes by capabilities and assigns appropriately. For example, GPU-accelerated tasks will only be sent to nodes with GPU resources. Newral’s scheduling component accounts for resource heterogeneity – CPU architecture, core count, GPU presence, etc. This is akin to BOINC’s plan classes or capability matching, ensuring tasks run on compatible nodes[41][42]. - Trusted Nodes: Over time, nodes with consistently good behavior might be flagged specially (having higher trust scores) and given preference or special roles (like handling critical tasks or fewer replications of results).
In summary, Newral’s architecture consists of a coordinated suite of microservices—orchestrator, trust management, data storage, event bus, etc.—and a multitude of participating nodes (agents). This design leverages modern cloud architecture principles to create a distributed computing platform that is scalable, resilient, and maintainable, while also layering in novel features like AI-driven orchestration and built-in trust management that differentiate it from prior systems.
AI Orchestrator Modes
A distinctive feature of Newral is its AI-enhanced task orchestrator, which can operate under different modes of autonomy and intelligence. These modes determine how much the orchestrator relies on artificial intelligence for decision-making in scheduling and resource management. The platform defines four orchestrator modes: AI Off, Advisory, Partial, and Full, each representing a progression in AI assistance and automation. This multi-mode approach allows a gradual integration of AI into the system, ensuring reliability and giving operators flexibility.
•	AI Off: In this mode, all AI-driven functionality is turned off. The orchestrator behaves in a traditional rule-based or manual way. Task scheduling decisions are made using fixed algorithms (like round-robin, simple heuristics, or first-come-first-served) or direct human input, without any AI optimization or learning. Essentially, AI Off is the baseline mode that prioritizes predictability and operator control. This mode is expected to be used initially (during MVP and testing) to validate the system’s core functionality and to serve scenarios where deterministic behavior is required. In AI Off, an administrator might manually set scheduling policies or intervene in task assignment. The absence of AI influence makes the system’s behavior easier to reason about, which is useful for debugging and for scenarios where AI might not yet be trusted.
•	Advisory Mode: In the Advisory mode, the orchestrator runs AI analytics in the background and generates recommendations or insights, but does not automatically enforce them without approval. Think of it as the AI acting as a co-pilot or decision support system. For example, the AI might analyze historical data and current system state to suggest: “Task A could run faster on Node 7 (due to its past performance and current load)” or “We predict Node 3 may become a bottleneck; consider moving some of its tasks elsewhere.” These suggestions can be presented to a human operator via the dashboard or simply logged for transparency. The operator can then choose to apply suggestions or ignore them. Alternatively, in a semi-automatic setup, the system might automatically apply some safe optimizations but still require confirmation for major changes. Advisory mode introduces AI-driven insights while keeping a human in the loop, ensuring no critical automated changes happen without oversight. It’s a low-risk way to incorporate AI: the AI helps optimize and catch issues (like spotting an inefficient task allocation or predicting a deadline miss)[43], but final control remains manual. Over time, as confidence in the AI’s recommendations grows, the team can shift to more automated modes.
•	Partial Mode: Partial mode enables the orchestrator to automatically handle certain decisions using AI, while other decisions remain rule-based or operator-controlled. It is a hybrid automation approach. The exact boundary can be configured:
•	For instance, the AI might be given control over routine optimizations such as load balancing tasks across nodes or rescheduling tasks from an overloaded node to an underutilized one. The AI would make these moves on its own, based on learned performance models, without waiting for human approval (thus faster reaction).
•	However, the orchestrator might still leave strategic decisions like task prioritization or project-level scheduling policies to the operator or a fixed policy.
•	Another example: The AI could automatically decide to replicate a task on an additional node if it predicts the originally assigned node is slow or might fail (thus improving reliability), but the AI won’t, say, decide on its own to drop tasks or change user-defined priorities.
Partial mode is analogous to advanced driver-assistance in cars: the AI handles certain tasks (staying in lane, adjusting speed in traffic) but the human is still driving overall. In Newral, Partial mode could manifest as the orchestrator autonomously taking care of known problem patterns (like restarting tasks on failure, reallocating when resource usage is higher than expected) – essentially implementing many best practices automatically. The operator’s workload is reduced since the AI addresses commonplace issues proactively. This mode begins to realize efficiency gains and faster responses (e.g., quicker mitigation of node failures) while maintaining human oversight on high-level operations.
•	Full Mode: In Full AI mode, the orchestrator operates with a high degree of autonomy, leveraging AI for nearly all aspects of orchestration. The orchestrator’s decisions (task scheduling, node selection, load distribution, scaling, etc.) are primarily driven by AI models and algorithms, with minimal to no human intervention required during normal operation. This is akin to a fully self-driving mode for the distributed system. The AI in Full mode uses a broad range of data:
•	It might use predictive models (trained on historical task execution times and node performance) to assign tasks to the optimal node, anticipating where they will complete fastest or most reliably[43].
•	It could manage resources dynamically, e.g., automatically pausing certain lower-priority tasks if it predicts a high-priority task needs more CPU on the cluster.
•	The orchestrator could also decide when to launch new nodes (in a cloud scenario) or invite more participants if the load is high, and conversely release resources when idle, achieving cost optimization.
•	In terms of trust, the AI might factor in complex trust scores and even perform anomaly detection on node behavior in real time, perhaps moving tasks away from a node that starts behaving erratically (beyond what simple rules would catch).
Essentially, in Full mode the orchestrator becomes an intelligent agent managing the platform. Human operators shift to a supervisory role, mainly monitoring the AI’s performance and handling exceptions or providing feedback for learning rather than micromanaging tasks. This mode promises maximum efficiency and responsiveness – tasks are distributed in an intelligent, data-driven way to prevent bottlenecks and maximize throughput[44][45]. However, Full mode also requires a mature and well-trained AI and significant trust in the system, which likely will come after extensive testing in Advisory/Partial modes.
The orchestrator modes can be switched or tuned as needed: - In early deployment, or for very critical computations, an admin may choose AI Off to have deterministic behavior or to adhere to strict scheduling policies. - As the system collects performance data, Advisory mode can run to validate the AI’s suggestions against actual outcomes, building confidence. - Partial mode might then be enabled for certain routine functions that the AI has proven adept at (e.g., auto-scaling, minor rebalancing). - Eventually, with extensive validation, the system can move to Full AI mode to reap the full benefits of autonomy.
This phased approach is critical from a safety and reliability standpoint. It’s recognized even in AI operations literature that you should have an “AI-off switch” or varying levels of human oversight during an AI’s lifecycle. Newral embraces that: operators always retain the ability to dial down the AI if needed (for example, if the AI is making poor decisions under novel conditions, one can revert to Advisory or Off).
To illustrate, consider a scenario: - In AI Off, tasks are assigned round-robin. Node 5 gets overloaded while Node 2 is idle, but the orchestrator doesn’t adjust because it’s following a static scheme. - In Advisory, the orchestrator would flag: “Node 5 has too many tasks, consider shifting some to Node 2.” - In Partial, the orchestrator would automatically migrate a task or two from Node 5 to Node 2 once Node 5’s CPU usage crosses a threshold, without waiting for approval. - In Full, the orchestrator would have likely avoided the situation altogether by initially distributing tasks based on predicted loads, and if not, it would continuously fine-tune assignment so that overloads either don’t happen or are quickly resolved, all autonomously.
Importantly, all modes adhere to core scheduling logic; AI augments it. For critical tasks, even in Full mode, the orchestrator will still respect any constraints (like a task must run on a GPU node, or must finish before a deadline) – the AI’s job is to meet those constraints in the best way possible, not override them arbitrarily.
In practice, the AI orchestrator might employ machine learning techniques such as reinforcement learning (training an agent to maximize throughput or minimize task completion time), supervised learning (predicting task runtimes[43] or node reliability), or heuristics derived from combinatorial optimization. These are complex and will be gradually integrated.
To summarize, Newral’s orchestrator can function on a spectrum from no AI involvement to fully AI-driven. This flexible design ensures that: - The system can be run in a conservative, fully controllable manner when needed. - AI can be introduced safely and its benefits realized progressively. - Ultimately, the platform can achieve a high level of automation and intelligence in orchestration, optimizing resource use and performance in ways static algorithms cannot, while maintaining the ability to fall back to simpler modes if anything goes awry or when human oversight is desired.
Security and Trust (Dr. Mann#n System)
Operating a distributed network of volunteer or untrusted nodes introduces significant security challenges. Newral addresses these through an integrated Security and Trust Management System, nicknamed Dr. Mann#n. The goal of this system is to ensure that participants (nodes) behave correctly and that the platform is resilient against malicious or faulty actors. It establishes who can be trusted with what, and implements safeguards to maintain the integrity of computations and data across the network.
Key aspects of security and trust in Newral include:
Node Authentication & Secure Communication: Every node that joins the network must authenticate itself. Newral uses cryptographic credentials – for instance, each node might have a unique key pair or certificate issued by the platform’s authority. Upon registration, a node obtains a certificate signed by the platform (or uses OAuth token exchange, etc.). This ensures that tasks are only assigned to legitimate nodes, and prevents unauthorized systems from impersonating nodes. Moreover, all communication between nodes and the orchestrator (and other services) is encrypted, likely via TLS, to prevent eavesdropping or tampering. This stops attackers from injecting false messages or reading sensitive data. By having a secured channel, we also ensure that result data returned by nodes hasn’t been altered in transit. In essence, strong authentication and encryption guard the entry points of the network and the data flow, which is a fundamental first layer of security.
Reputation-Based Trust Management: The Dr. Mann#n system implements a reputation model to quantify and track each node’s trustworthiness. Each node accumulates a reputation score based on its historical behavior: successful task completions, validated correct results, timely responses, etc., will increase reputation; whereas incorrect results, frequent aborts, or suspicious behavior will decrease it. This approach is aligned with reputation-based trust models in distributed systems, which “encourage honest behavior by rewarding trustworthy nodes and penalizing dishonest ones.”[7] Over time, the platform builds a profile for each node. A high-reputation node is considered trusted because it has a track record of good behavior, while a low-reputation node is treated cautiously. The trust score influences platform decisions: for example, security-sensitive or non-replicated tasks will be scheduled predominantly on high-reputation nodes, as they are less likely to cheat or error, whereas new or low-rep nodes may first be given tasks with built-in verification or parallel redundancy until they prove themselves.
Initial Trust and Dr. Mann#n’s Role: When a node first joins, it has little or no reputation. Initial trust might be based on other factors, such as identity verification (maybe certain nodes are pre-approved by an admin) or sandboxing tasks for new nodes. Dr. Mann#n could incorporate techniques from trust frameworks, like giving a small amount of initial trust to new nodes (a probation period). It continuously evaluates trust through metrics: - Direct observation: It monitors each node’s direct outputs (did it return the correct result for tasks where the outcome is known or replicated?). - Indirect observation: It listens to feedback or complaints (e.g., if a majority of other nodes disagree with this node’s result, that’s negative feedback). - Statistical analysis and machine learning: Patterns of behavior might be analyzed – e.g., if a node suddenly starts returning results much faster than usual or identical to another node’s (potential collusion), it flags this. - Trust revocation: If a node is identified as malicious, Dr. Mann#n can suspend or ban it from receiving new tasks[46][11].
This trust management is fundamental to prevent malicious behavior and foster cooperation[47]. It means the system isn’t blindly open – it’s earned trust.
Node Attestation and Integrity: Newral might employ mechanisms to ensure the node’s runtime environment is secure. For example, remote attestation (common in distributed networks with security concerns) could be used: nodes prove they are running an untampered client software (perhaps via secure enclaves or digital signatures on the client binary) and not a modified version that cheats. The Dr. Mann#n system could require such attestation at connect time. Also, whenever the platform sends executable code to a node (task binaries), it uses cryptographic hashes and code signing to verify integrity on the node side[8]. In BOINC, for example, “the validity of downloaded files is checked using hashes and code signing”[8]. Newral adopts similar measures: each task’s code or Docker image is signed by the project. The node agent will verify this signature before execution, ensuring no malware injection by a Man-in-the-Middle. This protects the node from running untrusted code and protects the platform from nodes running altered code that might always claim success without computing, etc.
Sandboxing and Node Security: Security isn't just about protecting the platform from bad nodes, but also protecting nodes from potentially harmful tasks. Nodes run tasks in a sandboxed environment to contain any malicious or faulty behavior of tasks. For example, tasks may be executed inside a Docker container or virtual machine with limited access to the host system (as BOINC introduced with VirtualBox to sandbox untrusted applications)[23]. This ensures a task can’t, say, read or modify the node owner’s personal files or infect their system. Dr. Mann#n system likely sets the security policy for this execution sandbox – such as what system calls are disallowed, capping memory usage, preventing network access from tasks (unless needed for the project and allowed), etc. This protects participants and thus encourages more to join, knowing their machines are safe.
Node Trust Levels and Certifications: Dr. Mann#n may assign trust levels or certifications to nodes. For example, after X validated results, a node might earn a “Trusted” badge or level 1 trust certification. At higher levels, maybe the node has proved reliable across different task types, so it's allowed to work on tasks solo (without replication) more often. These trust levels are akin to progressive privileges: a brand new node might initially get only redundant tasks (for verification purposes), while a certified node could get unique tasks. This concept is similar to adaptive replication in volunteer computing: BOINC identifies hosts that “consistently compute correct results” and trusts them by reducing replication[15]. Newral formalizes and automates this: trust is continuously maintained by Dr. Mann#n, such that the majority of work can eventually be done by a core of trusted nodes with minimal overhead, achieving efficiency while still catching outliers with periodic checks.
Misbehavior Detection and Response: The system actively looks for signs of cheating or malfunction: - Result verification failures: If a node’s result is frequently wrong (detected by our validation process), that’s a clear red flag. - Statistical anomalies: If a node completes tasks far faster than expected in a suspicious way (could be returning bogus results quickly), Dr. Mann#n will notice the anomaly. Similarly, if two nodes always produce the same wrong answers, they might be colluding; reputation systems can catch collusion by using algorithms that cross-compare feedback (like eigenvector-based trust models, etc.). - Security incidents: If a node tries to deviate from protocol (e.g., sending malformed data, attempting unauthorized actions), it’s a sign of a compromised or malicious node. The system will likely revoke its trust (quarantine or disconnect it)[48].
Newral might implement a strike system: a few minor faults might just lower rep, but a major incident (like returning a clearly fraudulent result) could result in immediate blacklisting. Conversely, good behavior over a long time restores trust gradually.
Sybil Attack Mitigation: A major threat in open systems is a Sybil attack (one actor creating many fake nodes to game the reputation system). Dr. Mann#n addresses this by making it costly to gain reputation – new nodes start with minimal trust and must spend significant computational effort correctly to earn it. Even if someone floods the system with 100 new nodes, those nodes will be sandboxed to trivial tasks at first and heavily validated, limiting any damage. Additionally, the system could require proof-of-work or some stake (if tokens are involved) to register new nodes, further discouraging mass Sybils. Reputation systems are known to help against Sybil attacks since fake identities can't easily accumulate good history without real work[49][12].
Transparency and Auditing: The trust system likely maintains logs of node performance and decision rationales (like why a node’s trust was lowered). There might be an interface for admins to review trust decisions, audit node histories, and even override if necessary (for example, if a good node was penalized due to a transient error, an admin could reinstate it). This transparency fosters user confidence that the system is fair. It also means if a participant feels wrongly accused, their case can be reviewed – a helpful community aspect.
End-to-End Data Integrity: Security extends to ensuring that results delivered to the end user are authentic. The platform might sign the final results with a platform key, indicating that the results have passed all verification and come from trusted processes. This is analogous to how distributed ledgers use signatures to guarantee data integrity. While Newral isn’t a blockchain, it could borrow the idea: attach a certificate of validation to outputs. For example, after a task result is validated, the orchestrator could produce a small certificate (including checksums of input, output, and a statement “computed by node X, verified by node Y and Z”) signed by the platform. This gives external consumers cryptographic confidence in the results’ integrity.
In conclusion, the Dr. Mann#n trust system in Newral provides a multi-layered defense: 1. Prevent entry of bad actors via authentication and encryption. 2. Contain potential damage via sandboxing on nodes. 3. Detect and discourage bad behavior via reputation and validation (rewarding good nodes, isolating bad ones)[7]. 4. Maintain reliability in a dynamic environment by updating trust assessments continuously and reacting to changes[11]. 5. Safeguard participants by ensuring tasks are safe to run on their machines (signed code, limited privileges).
This comprehensive approach aligns with best practices in distributed systems security, where trust is not binary but earned and quantified[12]. By implementing these measures, Newral aims to create an environment where nodes can cooperate without fear—neither of being exploited by the platform nor the platform being exploited by them—thus fostering a secure and trustworthy distributed computing community.
Reputation and Rewards
In Newral, reputation and reward mechanisms play a crucial role in motivating participants and maintaining the integrity of the network. While the previous section touched on reputation as a trust metric, here we delve deeper into how the platform uses reputation to influence node behavior and describe the reward system that incentivizes nodes to contribute computing power honestly and efficiently.
Reputation System Design: Each node in Newral has an associated reputation score (or potentially a set of metrics) that reflects its historical performance and reliability. The reputation system is essentially an implementation of a Reputation-Based Trust Model, which is known to be effective in large dynamic networks[16]. Nodes accumulate reputation based on: - Task success rate: Completing tasks correctly and on time increases reputation. Every time a node returns a result that is validated as correct (whether by replication, certificate, or audit), it gets positive credit. - Errors or Misbehavior: Returning incorrect results, failing to complete tasks (except due to reasons like task cancellation or system error), or violating protocols (security rules) will lower the reputation. For example, if a node’s result is frequently found to be wrong compared to others, it might have its reputation significantly reduced or even be set to a “probationary” status. - Consistency and Availability: Consistently being available (online, responsive) and providing stable performance might gradually boost reputation. Conversely, if a node often drops out mid-task or is offline when needed, it could impact its score (though the platform might handle availability separately to not penalize voluntary contributors harshly for downtime). - Feedback loops: The system could incorporate additional feedback, such as user ratings or project manager input, although in many volunteer computing contexts this is not common.
This approach of using past behavior to infer future trustworthiness encourages honest participation—trustworthy nodes are rewarded and dishonest ones are penalized[7]. It parallels how marketplaces like eBay or community forums use user ratings to build trust[50], as well as how P2P networks manage nodes[51].
Reputation Influence on Scheduling: The orchestrator leverages these reputation scores in decision-making: - When assigning tasks, especially ones that are not going to be replicated for verification, the orchestrator will prefer high-reputation nodes (they are more likely to do it right the first time). Low-reputation nodes might either get simpler tasks or tasks where a second node is anyway doing the same work for verification. - If a node's reputation drops below a threshold, the system might temporarily stop giving it tasks or assign only trivial “test” tasks until it proves itself again. In contrast, nodes with excellent reputation may be given a larger share of tasks (to utilize their reliability effectively). - This dynamic effectively prioritizes proven reliable workers, enhancing the overall throughput and correctness, since fewer tasks will need redoing[15].
Reputation and Node Rewards: Newral’s reputation ties into the reward system by which participants are recognized or compensated for their contributions: - Points and Leaderboards: One basic reward mechanism (commonly used in volunteer computing like BOINC) is to grant credit points for completed work. Newral could assign a certain number of points per task or per CPU-hour contributed (perhaps adjusted by the significance of the task and whether it was validated correct). These points accumulate and do not directly affect trust, but serve as a measure of contribution. Leaderboards can be published where nodes or users see their rankings by points. While points are not monetary, they tap into gamification: volunteers are motivated to compete for higher standings or collect badges. - Reputation as Reward: In a sense, reputation itself is a reward. Achieving a high reputation unlocks benefits: the node might receive more tasks (thus more opportunity to earn other rewards), maybe more complex or interesting tasks, and it might earn community prestige (trusted contributor status). As reputation correlates with more work and potentially more credit, it inherently incentivizes good behavior[7]. - Badges/Achievements: The platform could award badges or levels visible to the owner (and possibly public in the community) for milestones like “100 tasks completed with 100% accuracy” or “5 consecutive tasks validated correct”. These serve as non-monetary rewards to encourage sustained contributions.
Monetary/Token Rewards: Newral may also incorporate monetary rewards or crypto-token incentives, especially if it aims to be a decentralized marketplace for computing (like Golem or iExec). If so: - Nodes could earn a cryptocurrency (or platform-specific credits convertible to currency) proportional to the work they do. Projects or clients would pay into the system per task, and the funds get distributed to the nodes who completed the tasks. - Reputation would factor into payments: the platform might pay a bonus to high-rep nodes or require less verification overhead (which indirectly means they can earn more since they do more useful work rather than redundant checks). - This system must be carefully designed to avoid perverse incentives; for instance, if cheating could yield money, the reputation system and validation must be strong to deter it. Projects like Golem have explored this: Golem’s reputation system is meant to ensure providers are reliable so requesters feel safe paying them[12]. In Golem, after a task is completed, the system distributes ratings to participants which influence their future opportunities[13]. - For MVP, Newral might not start with monetary rewards, but the architecture could allow plugging that in later. Initially, points and rep might suffice to motivate volunteer contributors, as in many scientific distributed projects.
Two-sided Reputation: If Newral ever allows external task requesters (like an open marketplace), it might consider reputations for them as well. In a decentralized market, not only do worker nodes need trust, but task creators/requesters might also be rated (did they provide well-formed tasks, did they pay promptly, etc.)[14]. This can prevent abusive tasks (like those trying to exploit node resources in unauthorized ways) or ensure requesters don’t post junk tasks. However, for a controlled platform focusing on known projects, this may not be necessary.
Community & Fairness: The reputation and reward system contributes to a sense of community fairness: - Honest participants see their contributions recognized (through high rep and points), which builds trust in the platform and encourages continued participation[52]. - Dishonest participants find that cheating doesn’t pay off — either they get no rewards or are removed. Knowing that “bad nodes” will be caught and penalized also reassures volunteers that the science or computations are not tainted by fake results, which is a motivation to contribute to a meaningful cause.
Reputation Decay and Recovery: The system likely includes a concept of reputation decay or forgiveness. For example, if a node was good for a long time but had a single bad result, it shouldn’t be permanently damned by one glitch. Reputation could be calculated with a time-decay so that recent behavior weighs more. This way, nodes can recover from past issues by sustained good performance. Likewise, if a high-rep node suddenly starts acting poorly, the swift impact on rep will limit how much damage it does; and if it was an anomaly, it can earn trust back. Some trust systems use Bayesian approaches or sliding windows to update trust, ensuring current reliability is reflected more than ancient history.
Integration with Validator/Orchestrator: The orchestrator and validation components feed into the reputation system. For instance: - When a validator confirms a node’s result was correct, it triggers a positive rep update. - If a validator finds a discrepancy and identifies a node’s result as wrong, it triggers a negative update for that node (and perhaps a positive one for the node that was correct). - If tasks are replicated, the platform might use credit assignment schemes (like quorum systems) to adjust rep. For example, if Node A and Node B computed the same task and disagree, once the truth is found, decrease rep of the one who was wrong, and possibly increase rep of the one who was right (particularly if it was a contentious scenario). - Over time, as noted, a node with high rep might not be replicated at all (gaining efficiency), whereas a low rep node might still always be paired with a trusted node for a while until it establishes trust. This dynamic approach is in line with the adaptive replication concept which moves the “factor close to one” for reliable hosts[15], i.e., eliminating the need to duplicate all their work.
Rewarding Long-Term Contribution: In volunteer projects, often a user’s accumulation of credit points is a source of pride and can be used to measure long-term contribution. Newral can maintain cumulative metrics (e.g., total tasks completed, total FLOPs contributed, etc.). This does not directly factor into trust (trust cares about correctness percentage, not just volume), but it’s part of reward. The platform might showcase total contributions by project or user, potentially encouraging friendly competition or recognition (like “Top 10 contributors of the month” events).
Preventing Gaming the System: The combination of reputation and verification is essential to prevent nodes from gaming the reward system. If rewards (points or tokens) were given blindly per task, a malicious node might spam fake results to try and earn credit. However, because Newral ties rewards to validated results and reputation: - Nodes effectively only get rewarded when their results are confirmed correct. Invalid results yield no reward and harm their rep. - High-rep nodes might earn more, but only because they have proven capable of correct work frequently – which in itself meant they legitimately earned prior rewards. - Reputation being public (or at least known to the node owner) helps self-regulation: participants are likely to try to keep their node’s reputation high for the benefits it grants, analogous to how Uber drivers strive to keep high ratings because low ratings could deactivate them.
Analogy and Real-world Reference: This strategy mirrors systems like eBay, Amazon’s seller ratings, or P2P networks where trust is built via reputation feedback[50]. It also parallels academic contributions scoring in projects like Folding@home or BOINC, where participants accumulate “credits” as a sign of their contribution to science.
In conclusion, Newral’s reputation and reward subsystem serves multiple purposes: - It incentivizes reliability: aligning a node’s interest (to earn rewards or status) with the network’s interest (to get correct results)[7]. - It provides a measure of merit that the scheduling logic uses to improve overall system reliability and efficiency. - It motivates continued participation through recognition (points, badges, leaderboard) and possibly tangible rewards (if monetized), thereby helping grow and sustain the distributed network.
The combined trust and reward model in Newral exemplifies how "honest behavior by nodes is encouraged by rewarding trustworthy nodes and penalizing dishonest ones"[7], echoing established knowledge in distributed trust management, and is key to forming a robust, self-policing community of compute providers.
Validators and Result Verification
One of the critical challenges in distributed computing is ensuring the correctness of results returned by remote nodes without re-executing every task centrally. Newral tackles this through a comprehensive result verification mechanism involving validators, certificates, auditing, and selective replication. The aim is to confirm results are accurate while minimizing redundant computations, thus preserving the efficiency gains of distribution.
Why Verification is Needed: In an untrusted network, nodes could return wrong results due to hardware errors, software bugs, or malicious intent (e.g., a node might try to skip computations and send a bogus answer to get credit). Without verification, a single bad actor can spoil the integrity of the entire computation. Traditional volunteer computing often addresses this by performing replicated computing – send each task to 2+ nodes and compare results. However, this approach, if done uniformly, can double (or worse) the computing cost; indeed, “basic replication-based validation reduces effective computing capacity by a factor of at least two”[15]. Newral’s strategy is to reduce and target replication intelligently and use other methods to verify correctness, thereby bringing that overhead as low as possible[15].
Role of Validators: A validator in Newral is an entity (could be a dedicated service or simply a role that some nodes take on for certain tasks) that checks the validity of task outputs. There are multiple techniques for validation: - Redundant Computation (Replication): The simplest method: assign the same task to multiple nodes and compare results. If all agree, assume it’s correct; if there's a discrepancy, determine which node(s) are wrong. Newral uses this sparingly: primarily for tasks executed by low-rep nodes or for random spot-checks. The orchestrator/validator might send a task to 2 nodes initially. If their results match, it’s validated. If they differ, a third node might be invoked (majority voting or tie-breaker) and the deviating node is identified as incorrect. This approach ensures correctness at the cost of extra work, so it’s reserved for when it’s needed (e.g., new nodes, highly critical tasks where even a slight chance of error isn’t acceptable). - Certificates of Correctness: Some computations allow for efficient verifiable proofs. A certificate is additional data produced by a node that can be quickly checked to confirm the result is correct without redoing the entire computation. For example: - In primality testing, a node can provide a primality certificate for a number claimed prime, which the validator can verify much faster than a full primality proof from scratch. - In linear algebra, a node that computed, say, a matrix multiplication could send random projections of the matrices to validate the result (a form of probabilistic checking). - If a task was “find a solution to X”, the node’s result is a solution that can be plugged back into the problem easily to verify (like solving a SAT problem – a satisfying assignment can be verified quickly even if finding it was hard).
Newral encourages tasks (when possible) to be formulated to produce such certificates or at least easily-checkable outputs. The validator service will verify these certificates. If the certificate checks out, the result is accepted with high confidence without needing a second execution. Certificates can drastically reduce verification cost; for example, verifying a prime certificate is orders of magnitude faster than primality testing a huge number from scratch.
•	Auditing and Spot-Checks: Instead of replicating every task, Newral can adopt a random auditing approach. It might not verify each result, but unpredictably select some results to verify (either by re-computation or cross-checking via another method). For instance, 5% of tasks from high-rep nodes could be re-run on a validator node. If all audits pass, it reinforces trust. If an audit finds an error, that’s a serious issue: it may indicate the node got complacent or turned malicious, and then stricter checks can be applied (or its rep slashed). This “spot check” strategy is common in crowdsourced environments, catching cheaters with some probability while saving effort on most tasks.
Additionally, if a particular node is suspicious (maybe its pattern of results triggers an anomaly detector), the system can increase the audit frequency for that node.
•	Deterministic Checks and Analytical Redundancy: For some tasks, there are ways to verify results partially without doing a full recomputation. For example, if the task is sorting a list, one can easily check the output list is sorted and is a permutation of the input (these checks are linear time, vs sorting which is N log N). Or if the task is computing a checksum, the platform might know some properties of the correct answer (like a modulo or parity) to quickly check. Newral’s validator might incorporate such consistency checks whenever possible.
•	Cross-validation by Subsequent Tasks: In workflows (DAG tasks), sometimes later stages can help validate earlier outputs. If Task B uses output from Task A, Task B might fail or produce obviously wrong data if Task A’s output was wrong. The orchestrator can detect these anomalies (if many tasks that depend on A start failing, maybe A was wrong). In some cases, two different computations can be done that should yield the same result, providing a cross-check (like computing something via two independent methods as a consistency check).
Adaptive Replication (Trust-Based): Newral’s validator system is adaptive – it doesn’t treat all nodes equally: - New nodes or low-rep nodes might by default trigger replication or audits on most of their tasks until a track record is built. - Established high-rep nodes might execute tasks solo or with minimal oversight, with occasional random checks. As described in the BOINC paper, “adaptive replication moves this factor close to one by identifying hosts that consistently compute correct results ... and use replication only occasionally for jobs sent to these hosts.”[15] Newral follows this adaptive approach using its reputation system as the indicator of consistency. - If a high-rep node starts giving a wrong result (detected by a rare replication or certificate failure), that’s a strong signal – its reputation will drop significantly, and subsequent tasks for that node will be subject to heavier verification until it redeems itself (if ever). - In effect, the system spends verification effort where it’s needed: “trust but verify” strategy – trust is earned to reduce overhead but not given unconditionally[15].
Quorum and Consensus: The validator when faced with conflicting results uses a quorum-based decision: in a replication of say 3 nodes, if 2 agree and 1 differs, assume the majority are correct (especially if those majority have higher rep). Some projects implement more complex consensus (like credibility weighting of votes by rep). Newral could potentially weight a result by the node’s reputation. For example, if a high-rep and a low-rep node disagree, the high-rep node is more likely to be correct. Nonetheless, to be safe, the platform might still run a tie-break computation on a fresh node to be absolutely sure, while penalizing accordingly.
Efficiency Considerations: The goal is to minimize repeated calculations while still catching errors. Newral’s approach includes: - Using certificates whenever possible (they eliminate repeat calc entirely for those tasks). - Using selective replication (based on trust levels) to avoid full double-calculation for trusted work. For instance, if 90% of tasks are run on trusted nodes without duplication, the overhead might only be like 10% replication, effectively boosting capacity versus naive 100% replication. The BOINC refinement indicated it can move the replication factor “close to one”[15], meaning almost no overhead. - Using audits which, if set to a low frequency for good nodes, incur only minor overhead but serve as deterrence against potential cheating. - Parallel verification: If replication is used, it’s done in parallel from the start for untrusted nodes so that time isn’t lost doing serial rechecks. E.g., assign two new nodes the same task simultaneously and compare results when both return, rather than assigning one then waiting then another (which doubles latency). - Caching Verified Results: Another tactic: once a computation is verified, cache it. If the same computation is requested later (or a sub-task of it), the system can reuse the result instead of recomputing. This is more applicable if tasks can repeat (or if subproblems overlap between tasks). By storing outputs with signatures that they were verified, Newral could skip future computations of identical tasks (like common library sub-calculations). This is analogous to memoization or a result database. However, in many volunteer scenarios tasks are unique; still, in some scientific projects (like searching for primes), overlapping checks might occur, and caching would help.
Validator Implementation: The validator could be a separate microservice or part of the orchestrator: - Possibly the orchestrator contains the logic to perform validation decisions and assigns “validation tasks” to nodes. For example, when a task is completed by Node A, orchestrator might decide “send this result for validation to Node B” (which then essentially re-runs the task or runs a checker on it). - Or there could be a dedicated pool of validator nodes (perhaps run by the platform operator for quality control) that re-run tasks as needed. These could be highly trusted machines (maybe controlled by the project owners) that the platform uses to validate a sample of results. That assures an unbiased check. - In a decentralized sense, any node could act as a validator for another node’s task. In fact, when tasks are replicated, the nodes are mutually validators for each other (whoever matches the majority is effectively validating the other). The system orchestrates this without needing a distinct "validator node" type.
Preventing Repeat Offenders: If a node is found consistently returning bad results, the validator system will flag it to Dr. Mann#n, which can then take action (reduce its reputation drastically or ban it outright). This means the platform doesn’t waste resources repeatedly verifying a node that has proven untrustworthy; it will either stop giving it tasks or only give it trivial work until it demonstrates improvement. This protects efficiency by pruning out or isolating bad actors quickly.
Example Workflow: Suppose a low-reputation Node X completes a task. The orchestrator, following policy, also had Node Y compute it in parallel. Validator compares: - If X and Y agree -> mark result as verified, both X and Y get credit and a small reputation boost. - If they differ -> orchestrator engages a trusted Node Z to compute it as tie-breaker. Say Z’s result matches Y. Then: - Accept Y/Z’s result as correct output. - Node X loses rep for being wrong; Node Y gains some rep (it was correct). - Possibly give Node Z a reward or just a validator acknowledgment. - The result is delivered with a note that it was validated by redundancy (maybe not user-visible but logged). - Node X might now be flagged; further tasks from X will all be replicated or withheld if X crosses a failure threshold.
Special Cases: Some tasks are nondeterministic or have multiple valid outcomes. In such cases, the platform’s validation must be appropriately defined. For example, if a task is to find any one solution to a problem (not necessarily a unique solution), two honest nodes might legitimately return different correct answers. The validator should then have a method to judge validity (perhaps by checking each answer satisfies a set of conditions rather than comparing them). Newral must account for this in its validation logic for certain task types (this is more domain-specific and might involve project-specific validators).
Result Certification to End User: After a result passes validation, the platform can mark it as certified. If we think of end-to-end trust, an end user receiving results might want proof they were validated. Newral could attach meta-data like “computed by Node17, validated by Node23” or just a general statement “Verified by Newral” possibly with a reference (like “Validation chain available on request”). This is important if, for instance, Newral were being used for paid computations – the clients want assurance the results are good. In a scenario of disputes (client says result is wrong), the platform has the validation logs and certificates to show the effort made to ensure correctness. It’s somewhat analogous to a quality audit trail.
Bottom Line: Through its validators and result checking strategies, Newral ensures computational integrity. Nodes cannot simply claim results and get away with it – the system either catches errors or has high confidence they wouldn't occur by virtue of trust management. By doing this intelligently (adapting to node reliability), Newral avoids the pitfall of losing too much efficiency on double-checking everything. As noted in distributed computing literature, robust result verification is essential for success[53][54].
With Newral's approach: - Most of the time, trusted nodes do work once and it's accepted. - New or untrusted nodes have their work checked, protecting the platform as they prove themselves. - The overall effective throughput of the network remains high because verification overhead is concentrated where needed, not uniformly everywhere[15].
This gives Newral the best of both worlds: the power of distributed computing and the reliability of centralized computing, which is achieved by carefully orchestrated validation.
Data Storage and Portability
Newral’s architecture separates concerns of computation and data management through a hybrid storage system combining a relational database with scalable object storage. This design ensures data integrity, efficient access to large datasets, backups, and portability of the platform across different environments.
PostgreSQL for Metadata: The platform uses PostgreSQL as a centralized relational database to store structured metadata and state information: - Task Metadata: records for each task including its ID, type, input references (object store keys or pointers), which node(s) it's assigned to, status (queued, running, completed), results (object store key or inline small result), and any dependency relationships (for DAG tasks). Also, tasks may have fields like estimated runtime, deadlines, etc., stored here for quick querying by the orchestrator. - Node and User Info: tables for node registry (node ID, authentication info, hardware capabilities, current reputation score, last check-in time, etc.). If Newral has user accounts or project accounts, those are stored with their credentials and permissions. - Reputation and History: logs of tasks completed by each node, success/failure counts, and reputation history. The trust system updates these tables when events occur. - Global Configurations: any system-wide settings, mode configurations (like current orchestrator AI mode), scheduling policies, etc., can reside here. - Transactional Integrity: PostgreSQL ensures strong consistency for updates. For example, when a task is marked completed and its result is stored, the orchestrator can update the DB in a transaction: mark task status = completed and insert a reference to result file, while also incrementing the node’s completed task count, etc. This all-or-nothing transactionality means the platform can recover from crashes without confusion (no ghost tasks stuck running, etc.).
Postgres is known for its reliability and has advanced features like stored procedures and triggers, which could be used for certain logic (though heavy logic likely stays in app code). It handles moderate volumes of data quite well and can scale vertically, or horizontally via partitioning or replication for reads. For Newral’s MVP, the load on Postgres is mainly metadata reads/writes, which are not extremely heavy (the bulk data isn't in the DB). It should easily handle thousands of tasks per second updates on a modern machine, especially with indexing and proper schema design.
Object Storage for Bulk Data: For handling large files and binary data, Newral employs an object storage service that provides an S3-compatible interface: - Input Data: Many distributed tasks involve input datasets (e.g., a chunk of a large file to analyze, images to process, numeric data, etc.). Instead of storing these in the DB or sending from orchestrator directly, they are stored as objects (with unique keys, such as projectX/input/12345). - Output Data: Similarly, results, which can be large (like the output of a simulation or a processed dataset), are stored in object storage. The orchestrator or node uploads outputs (like a file) to object storage and then just stores a reference (key/URL) in Postgres. - Intermediate Data: In multi-stage workflows (DAGs), intermediate outputs of one task become inputs to another. These, too, reside in object storage. For example, Task A’s output is saved with a key and that key is passed to Task B (via the DB entry or event message). When Task B runs, it retrieves that object. - Choice of Object Store: The platform might use a self-hosted solution like MinIO (which provides S3 API compatibility and can be run on the same cluster) or rely on cloud storage services (AWS S3, Google Cloud Storage, etc.) depending on deployment. The key is that it’s accessible to all nodes (perhaps via HTTP) and can scale in capacity and bandwidth. Object storage typically offers high durability and redundancy (S3 guarantees, for example, 99.999999999% durability by replicating data). - Decoupling compute from data: This design makes compute nodes stateless – they don’t need to keep data permanently. A node gets a task, downloads required input files from object store, does computation, uploads the output file, and can then discard the local data. If another node later needs the same input (for a different task or re-run), it fetches again from the store. This stateless approach is in line with cloud architecture best practices, where compute and storage are separate, allowing the system to replace or scale compute nodes without data loss[3].
Backup and Durability: - Database Backup: PostgreSQL data is regularly backed up (e.g., daily dumps or continuous archiving of WAL logs for point-in-time recovery). Because the DB holds critical metadata (like what tasks are pending or done), losing it could be catastrophic. So a backup strategy is crucial. Possibly a primary-replica setup is used for high availability, where a standby Postgres is updated in real-time to take over if primary fails. - Object Storage Durability: If using a service like AWS S3, durability is extremely high by default (multi-AZ replication). If using MinIO or similar, Newral would configure it with erasure coding or replication across multiple disks/servers so that hardware failures don’t lose data. Also, snapshots or versioning can be enabled in case of accidental deletion. - Data Transfer and Locality: The system tries to keep data transfer efficient. Large objects might be cached or delivered via CDN if nodes are globally distributed. However, initially, nodes likely connect to a central store. If network cost or speed becomes an issue, one might implement data locality: e.g., if tasks are assigned to nodes that already have needed data from previous tasks, or place nodes in regions and replicate object storage across regions so nodes fetch from the nearest location. This can get complex, though, and might be a future optimization.
•	Handling Big Data Use Cases: If a project has petabytes of data, sending it over the internet to volunteers is impractical. That scenario might need a specialized approach (like enabling nodes to bring tasks to data rather than data to tasks, or expecting volunteers to have a copy of the dataset). But those cases might be out of scope for MVP. Typically, volunteer computing tasks are self-contained or have moderately sized inputs (MBs to a few GB at most). Newral’s architecture is suitable up to that scale.
Stateless Compute & Recovery: Because persistent state is centralized, recovery from failure is easier: - If a node crashes mid-task, the orchestrator notes that (likely via missed heartbeat or error message). The task remains incomplete. The orchestrator can reschedule that task to another node because all needed data is still in object storage and task info in DB. - Even if the orchestrator itself restarts, it can consult the DB which has the authoritative list of tasks and their statuses to resume operations correctly. - There’s a concept: “compute nodes are truly stateless ... when a node crashes, it just asks Postgres where to start and pulls what it needs from S3”[3]. This is exactly the design Newral follows. For example, a new node can join and immediately start taking tasks, because all it needs is to fetch input files from given links and start computing.
Kafka and Data Storage: The use of Kafka ties into storage: - Kafka events might include references to stored data (like a task_completed event includes the key of the result object in storage). - Kafka itself is not used for storing payloads of tasks (we wouldn’t put a 100MB result in a Kafka message). Instead, Kafka messages carry small metadata and identifiers. This decouples heavy data flow from the event system, which is important for performance. - Kafka does persist its event log for a short period (like a buffer). But after tasks are done and state is written to DB, Kafka messages are not needed long-term. The DB and object store are the long-term memory of the system, while Kafka is more like short-term communication (though with retention for fault tolerance).
Portability and Cloud Independence: By using standard components (PostgreSQL, S3 API, Docker/Kubernetes for services), Newral can be deployed in various environments: - On-Premises: All components can run on organization’s own servers or cluster. - Cloud: It can be deployed to AWS, Azure, GCP etc. If on AWS, they might use AWS RDS for Postgres and AWS S3 for storage, and the rest on EKS (Kubernetes) – minimal changes required because interfaces are standard. - Hybrid: Possibly a central control in cloud with volunteers worldwide connecting via internet to fetch data from a CDN or bucket near them. - For developer convenience, one could even run a mini Newral on a single machine (database + MinIO + orchestrator, etc.) to test workflows.
If the platform needed to migrate from one environment to another (say from a test cloud to production cloud or from one cloud provider to another), the steps would involve: 1. Migrating the Postgres database (dump and restore). 2. Copying the entire object storage bucket data to the new location (tools exist for S3 to S3 copying). 3. Restarting services in the new environment pointing to the new DB and storage. Because the data layer is distinct and fairly self-contained, and uses widely compatible formats (SQL dump, S3 objects), migration is straightforward compared to if data were locked in proprietary systems.
Backup and Disaster Recovery: Regular backups of Postgres (daily or more often for point-in-time via WAL archiving) and of critical object storage (maybe snapshots or replication to another region) ensure that if a catastrophic event occurs (like complete datacenter failure), the platform can be restored elsewhere. - The object store might be inherently multi-zone, but if not, one could do periodic syncs to a secondary store. - The DB can have a read replica that’s kept up to date in another zone or region, which can take over if primary fails (with some minor lost recent updates if asynchronous replication is used). - Because tasks can be re-run, even if some recent state is lost, the system can recover by rescheduling tasks that are not marked completed in the recovered state (some redundancy in computing might be needed but better than losing final results unnoticed).
Local Caching and Transfer Optimization: - Nodes might implement local caching of frequently accessed data. For example, if a project sends the same large lookup table file to many tasks, an agent can cache it on first download (provided it has a stable path or ID) so that subsequent tasks reuse the local copy instead of re-downloading. The orchestrator could send along a content hash so the agent can check if it already has that content. This is more an agent-level optimization. It’s allowed by the architecture but not required. - Similarly, if tasks produce large outputs, perhaps intermediate compression is applied, etc., to reduce storage and transfer costs.
Observability of Storage: The platform should monitor storage metrics – how full is the object store, how much data transferred, etc. Possibly enforce quotas (if a project tries to store TBs, the platform might require special handling or deletion of old data after use). The DB also should be monitored for performance (index bloat, slow queries, etc.), but since it mainly handles small records, typical indexing should keep it fine.
Example Data Flow of a Task: 1. A project uploads input data file(s) to object storage (or they’re already present from initial dataset). 2. They submit a task referencing those file keys. 3. Orchestrator creates a task record in Postgres with state = queued, input_keys = [list]. 4. Node picks up the task (via Kafka event or API call). It queries Postgres or gets from the event the input file keys/URLs, then downloads them from object storage to local disk. 5. Node executes computation. 6. Node uploads the result file to object storage, gets back a key (maybe it set the key or storage returns one). 7. Node reports task complete to orchestrator, including output key (and possibly a computed hash or certificate). 8. Orchestrator/validator verifies output if needed, then updates Postgres: state = completed, output_key = (the storage key), validated = true. 9. The project or next dependent task is notified and can retrieve the output from storage using that key.
Throughout, the actual heavy files moved only between node and storage, not through orchestrator, making the system scalable. The orchestrator and DB handle only small metadata like keys and statuses.
This approach exemplifies the design used by many modern distributed systems (for example, Hadoop and Spark separate HDFS storage from compute tasks, or cloud functions pulling data from cloud storage). It ensures smooth operation and data persistence independent of transient compute node life cycles.
In summary, Newral’s storage strategy – PostgreSQL + Object Store – provides: - A robust, consistent metadata backbone, with the relational database supporting crucial coordination and record-keeping[55][2]. - A scalable, reliable bulk data repository via object storage, with high durability and throughput for distributing task I/O. - The ability to backup and move the entire system’s state relatively easily, aiding in recovery and migration. - A stateless compute paradigm that simplifies node management and enhances fault tolerance[3]. This storage architecture is a key foundation ensuring Newral can handle data gracefully as the platform scales and evolves.
Event Bus (Kafka) Integration
Newral employs an event-driven architecture facilitated by an Apache Kafka event bus to coordinate and decouple the actions of its microservices. Kafka serves as the central nervous system for message passing, ensuring reliable, scalable, and asynchronous communication between components of the platform.
Publish/Subscribe Model: In Newral, services do not call each other directly for most operations; instead, they communicate by producing and consuming events on Kafka topics. For example: - When the orchestrator assigns a task to a node, it produces an event like task.assigned with details (task ID, node ID) to a Kafka topic, say tasks or task_events. The target node (or rather, the agent service managing nodes) is subscribed to that topic (or a partition or consumer group keyed by its node ID) and will receive the assignment message. - When a node starts and then completes a task, it produces task.started and task.completed events to Kafka (perhaps to a task_updates topic). The orchestrator service listens for these to update its state and possibly trigger follow-up actions (like scheduling dependent tasks). - The validation service might subscribe to task.completed events too. If a result needs verification, it could consume the event and initiate a verification process or produce a task.verify.request event. - The reputation service listens to task.verified or task.result.valid / task.result.invalid events to update node scores.
This pattern means services are loosely coupled: the producer of an event doesn’t need to know which specific service will act on it, if any[18]. Many services can listen to the same event type, each doing their part (multi-subscribing). This decoupling improves modularity and makes it easier to add new features – for example, adding a logging service that records all task.completed events to a database for analysis requires no change to orchestrator or agents; it just taps into Kafka.
Decentralized Orchestration via Kafka: Kafka can effectively act as a distributed task queue: - The orchestrator may simply post tasks to a topic and nodes fetch them. This is akin to a work queue model. If orchestrator uses keying, it can direct tasks to specific nodes by using their node ID as a message key, so the Kafka consumer group ensures that node’s consumer gets the message. - Alternatively, orchestrator explicitly sends a direct message (with node ID key) as above. Kafka ensures the message is delivered to that node’s consumer partition, preserving order for that node’s tasks if needed. - Kafka’s asynchronous nature means services don’t stall waiting for responses. The orchestrator can fire off assignments and continue, then later react when the completion event comes. This improves throughput and resilience: if, say, the orchestrator briefly goes down, tasks in progress still get done and their completion events will wait in Kafka until orchestrator is back to consume them.
Loose Coupling & Scalability: - Because producers and consumers are independent, each service can scale out by adding more instances consuming a topic. For example, if one validation service can’t keep up with all task.completed events (maybe heavy certificate checking), multiple instances can consume in a consumer group and Kafka will load-balance event partitions among them. - Kafka itself is highly scalable: it can handle huge message volumes by adding brokers and partitions. It replicates data across brokers for fault tolerance[19]. In context, if Newral had tens of thousands of nodes sending events, Kafka can partition topics (e.g., by task or node ID) across multiple servers to handle throughput. - High throughput and low latency: Kafka’s design (sequential log, zero-copy, etc.) allows it to process on the order of hundreds of thousands of messages per second on modest hardware. This means the overhead of using Kafka for orchestration is small relative to the actual computations being performed. It introduces maybe a few milliseconds of delay in message transit, which is negligible in most distributed computing contexts (where tasks often run for seconds to hours). In return, it provides durability and asynchronicity.
Durability and Fault Tolerance: Kafka stores messages for a configurable retention period. If a consumer (service) goes offline, it can come back and read from where it left off (using committed offsets). For example, if orchestrator crashes after sending assignments but before receiving completions, when it recovers it will read from Kafka any task.completed events it missed and update the system state accordingly. This prevents event loss – a known benefit of Kafka’s design as opposed to transient messaging or direct RPC. It “guarantees data durability even in the face of failures”[5] because messages are replicated and persisted to disk.
Event-Driven Behavior Examples: - Real-time updates: If Newral has a dashboard showing task progress, that UI can get updates via Kafka too (perhaps through a WebSockets server that subscribes to Kafka topics internally and pushes to web clients). The UI then shows nearly instant updates when tasks start/finish. - Node status events: Nodes might send heartbeat or resource usage events (node.status) at intervals. Kafka can carry these, and the orchestrator or a monitoring service consumes them to detect slow or stuck nodes and trigger reassignments or alerts. - Complex workflows: For DAG support, when a task completes, orchestrator (or a workflow manager service) sees the event and then checks dependent tasks. Once all dependencies are done (tracked in DB or via counters events), it emits events to launch the dependent tasks. This asynchronous chain fits well with Kafka, as it’s essentially a stream of events triggering new actions.
Ordering and Consistency: Kafka provides ordering within partitions. Newral must choose partition keys carefully to get the required ordering. For instance: - We may partition by task ID for events about that task, so all events for a specific task go to the same partition and thus stay in order (task.assigned -> task.started -> task.completed). Alternatively, use node ID as key: all events relevant to a particular node (tasks assigned to it and its responses) are ordered. That might be more crucial for commands to a node (to not send two tasks out-of-order). - System-wide ordering (across all tasks/nodes) is not needed and not practically achievable at huge scale, but it’s also not needed for correctness. We only need relative ordering for related events. For independent tasks, concurrency is fine.
Asynchronous Processing Advantages: - Non-blocking workflow: The orchestrator can handle thousands of tasks concurrently because it's not waiting in a tight loop for each event; Kafka buffers events and orchestrator processes them when possible. If orchestrator gets overwhelmed, the Kafka queue will build up, but not drop messages, giving orchestrator a chance to catch up. This decoupling means temporary overloads don't crash the system; it just processes with some lag. - Failure isolation: If one component is down, others can still produce events and those events will be stored until the component comes back, as long as Kafka is running. This is far better than direct RPC where a down service would cause immediate failures in any service trying to call it.
Kafka as Event Store: Kafka's retention might be configured so that events stick around long enough for recovery, but maybe not forever (to avoid infinite storage growth). It might keep, say, a few days of events. That is enough for short-term recovery and debugging. For long-term audit logs, the platform could either extend retention or pump events to a long-term database (like a data warehouse or cold storage logs). Tools in the Kafka ecosystem (Kafka Connect, etc.) can stream events to storage or monitoring systems in real time.
Integration with Microservices Ecosystem: Apache Kafka integrates well with microservices and modern frameworks. We can use schema registries (e.g., Confluent Schema Registry with Avro/Proto definitions) to ensure producers and consumers agree on event data formats[56], which reduces error. Also, Kafka allows adding new event types or topics without interfering with existing ones, which aids platform evolution.
Example Event Topics in Newral: - tasks.assign (with key=NodeID): Orchestrator -> Node: “here’s a task for you.” - tasks.status (with key=TaskID): Node -> Orchestrator: could include started, progress updates, completed. - tasks.result (with key=TaskID): Node -> Orchestrator/Validator: contains result meta (object key, checksum, maybe snippet). - tasks.verify (with key=TaskID): orchestrator/validator -> verification process or node: e.g., ask a node to re-run (if we treat that also via events). - node.heartbeat (key=NodeID): Node -> Orchestrator: “I’m alive at time t with X resources free.” - node.join / node.leave: Node -> Orchestrator (via events, or orchestrator might also register node when it connects via an API). But could be evented for logging or triggers (like auto tasks assignment to new node after join event). - reputation.update (key=NodeID): Trust service -> DB or to notify orchestrator if a node’s status changed significantly (maybe orchestrator would like to know if a node fell below a threshold to avoid giving it big tasks). - system.alerts or system.events: various system-wide events possibly for admin monitoring (like “Kafka lag high” or “DB backup completed”) – more devops oriented.
Kafka and Event-Driven Benefits Recap: - Decoupling: Services (or microservice instances) don't need direct knowledge of each other, just of event formats. They can be developed and scaled independently[39]. - Real-time processing: The system reacts to events as they occur, enabling dynamic adjustments. For instance, a node leaving (crash) can immediately trigger orchestrator to reschedule tasks because it saw a “node.leave” event or missed heartbeats. - Extensibility: New features often just mean new event types or new consumers. E.g., if in the future Newral integrates a Machine Learning analytics service to predict node reliability, that service could consume historical events from Kafka (via Kafka Streams or a connector) and produce recommendations events that orchestrator subscribes to. This can be added with minimal disturbance to existing components.
Comparison to Traditional RPC/DB Polling: Without Kafka, orchestrator might have to poll the DB for status changes or maintain WebSocket connections to all nodes to hear completions, etc. Polling DB frequently would add load and possibly miss instant reactions. Maintaining thousands of direct connections to nodes is possible but more complex. Kafka provides a more robust and scalable pipeline where nodes just fire-and-forget their messages to Kafka, and orchestrator picks them up reliably. It’s effectively a distributed queue system tailored to scale.
Operational Considerations: Running Kafka requires managing a cluster of brokers and Zookeeper (though newer Kafka can be run without ZK with KRaft mode). The platform must monitor Kafka’s health, disk usage (as it's storing logs), and tune retention and partitions as system grows. But since Kafka is a mature tech, many tools and best practices exist. The benefits it gives in reliability and scaling of communication are typically worth the overhead in a system like Newral that will handle a large volume of asynchronous events.
In summary, Apache Kafka provides Newral with a fast, reliable event bus that orchestrates the flow of information: - It keeps microservices loosely coupled and highly coherent through events[4]. - It enables the system to react to events in real time, forming the backbone of the event-driven architecture. - It ensures that critical messages (like task completions) are not lost, and can queue up if any component is temporarily slow or down, leading to a more resilient system[5]. - It supports horizontal scaling of services and overall throughput by adding brokers or partitions as needed.
This event-driven approach with Kafka is pivotal for achieving high performance and reliability at scale in the Newral platform, aligning with modern best practices for distributed system design where an event bus often acts as a central communication hub[20][21].
Client Side: Metrics, Security, and Constraints
While much of Newral’s design focuses on the server-side infrastructure, the client-side software (agent) on each participant node is equally important. The client agent is responsible for executing tasks efficiently, gathering metrics, enforcing local constraints set by the node’s owner, and ensuring the node’s security and usability aren’t compromised by participating in Newral. Essentially, it bridges the platform and the volunteer’s machine.
Metrics Collection and Reporting: The agent continuously monitors its host’s resource usage and the status of tasks. Key metrics include: - CPU usage: how much CPU time the Newral tasks are consuming vs. idle. This can be reported as a percentage or in terms of seconds of CPU time used. It helps both the user (to see impact) and the orchestrator (to avoid overloading the node). - Memory usage: how much RAM the tasks are using. The agent may enforce a cap (if user set one) and also report usage to orchestrator if tasks risk exceeding node capacity. - Disk and I/O usage: if tasks involve reading/writing large files, the agent might report how much disk is used (especially if an input was cached) or if disk is nearing full. - Temperature or battery (for laptops): some advanced metrics, if accessible and relevant (to avoid overheating or battery drain). - Task progress: Many tasks can provide progress info (like “50% done” or iteration count). The agent can forward this to the orchestrator or UI so the user sees that tasks are making headway.
These metrics are sent to the orchestrator or a monitoring service, possibly at regular intervals or on significant changes. Real-time metrics allow dynamic scheduling decisions – e.g., if a node is running hot or CPU is fully used, the orchestrator might hold off sending new tasks[22]. It also allows detection of anomalies (task hung or memory leak, etc.).
On the user side, the agent likely provides a local interface (GUI or command-line) for the user to see metrics: - e.g., “Running task #123 (Project: PrimeSearch) – 40% complete, using 2 CPU cores at 80% each, 500MB RAM.” - Or aggregated stats: “Newral using 30% of CPU, has completed 5 tasks today, earned X points.”
Task Control (Start/Stop/Pause): The agent can suspend or resume tasks in response to user actions or system conditions[22]. For example: - If the user has configured “Do not run when I’m using the computer,” the agent will pause tasks when user activity is detected (mouse/keyboard or active foreground app) and resume when the machine goes idle. BOINC does this via a configurable idle period. - If CPU throttling is set (say 50%), the agent will implement it by running tasks for, say, 0.5 seconds then sleeping 0.5 seconds in a cycle (this effectively yields ~50% usage)[24]. - The agent handles task preemption gracefully: it can checkpoint tasks if the application supports it (periodically save state to disk) so that if paused or interrupted it can resume later without losing all progress. Not all tasks have checkpointing, but if they do (e.g., long simulations often checkpoint), the agent coordinates that. - The agent is also able to abort tasks if needed (say, user requests it, or the platform tells it to cancel due to task no longer needed or deadline passed). It then frees the resources.
This control is crucial for user satisfaction; Newral should not degrade the user’s experience of their own machine. It must operate unobtrusively in the background, yielding resources whenever the user needs them.
Local Security and Sandbox: The client agent ensures that running a Newral task doesn’t expose the host system to risk: - Sandboxing: Each task runs in a restricted environment. This could be achieved by: - Running the task process under a limited user account with no admin rights, and with access only to specific directories (like a sandbox folder for tasks) – preventing it from reading/writing outside. - Using OS-level sandbox APIs or containerization. For example, tasks could be executed in a Docker container or similar. In some volunteer computing systems, VMs are used for strong isolation[23]. Docker might provide a good balance: it can restrict CPU/MEM usage, file system access, and network access. - Tools like seccomp on Linux could restrict system calls the task can make, limiting, say, no ability to open network sockets (unless the project specifically needs outbound net access, which most volunteer tasks do not for security reasons). - Code Signing and Integrity: The agent verifies the integrity of any task code it downloads before execution. As mentioned, tasks are accompanied by cryptographic signatures[8] or hashes from the server. The agent checks that the binary or script matches the expected hash and is signed by a trusted key (the project’s key which the agent knows via configuration). Only then does it run the code. This prevents tampering in transit or a malicious server from sending unauthorized code. - Resource Limits: The agent will impose limits to prevent tasks from monopolizing the machine: - CPU usage as configured (or default, e.g., use at most N cores or only run when CPU idle). - Memory limits: if a task tries to allocate beyond a threshold, the agent could terminate it or warn, to avoid swapping the whole system. - Disk usage: the agent might enforce a limit on how much disk space Newral can use for all data (user might set, e.g., “max 5 GB disk” in preferences). It will then clean up older files, or refuse new tasks that would exceed that[25]. - Network usage: user might set a cap like “don’t use more than 1 GB/day of bandwidth” – the agent can track how much it downloads/uploads and slow or stop transfers to respect this[25].
•	No interference with user: The agent likely runs tasks at low priority (nice level). BOINC, for example, runs science apps at the lowest priority so that if user programs need CPU, they preempt the science app easily[29]. Similarly, GPU tasks, which can cause UI lag, are often paused if the user is actively using the GPU (like moving the mouse or using a 3D app). Newral’s agent can adopt similar strategies.
User Preferences and Constraints: Newral recognizes that volunteer participants will have preferences on how their resources are used. Common configurable options (with their typical reasons) include: - CPU Throttle/Usage Limits: e.g., “use at most 60% of my CPU” or more fine-grained “use at most N out of M cores” or “only when CPU idle above 75%”. This prevents the machine from getting too sluggish or hot. The agent enforces this by either limiting concurrency (like only start tasks on 2 out of 4 cores) or by duty cycling as mentioned[30]. - Runtime Schedule: e.g., “only run tasks between midnight and 7am” (user might not want extra load during work hours)[31]. Or the inverse, “don’t run when on battery power” (common for laptops, to not drain battery). - Memory/Disk/Network Limits: as mentioned, user can define how much of each to allow[25]. The agent and orchestrator coordinate on disk space: e.g., orchestrator might not send huge tasks if the node’s declared disk limit is small. - Project Preferences: If multiple projects run on Newral (like mathematics vs biomedical), a user might opt out of some or prioritize one. Newral can allow opt-in to types of tasks. For instance, a user might say “I only want to support Project X” – then the orchestrator will only assign that node tasks from Project X. This is akin to BOINC’s project selection or using keywords to tag tasks and user interests[26][27]. - Concurrent Task Limit: Possibly, allow user to set “at most 1 task at a time” even if they have 8 cores. They might do this to minimize memory use or interference with other things. - Use of GPU: Some may allow CPU but not GPU (or vice versa) in case GPU tasks make the system lag or they want to reserve GPU for other use. - Consent to use network: maybe a user can specify if their machine can do heavy networking (some might have data caps).
The agent must abide by these preferences. If orchestrator tries to give tasks exceeding a limit, the agent may reject or delay them. Actually, the agent can advertise its capabilities to orchestrator (like in the account manager or initial handshake, send: “I have 4 cores, allow use of 2, have 2 GB RAM free, 5 GB disk allowed, etc.”). The orchestrator then schedules accordingly. Or if orchestrator is not aware (maybe it just pushes tasks), the agent will simply not run tasks that would violate constraints – perhaps putting them on hold or returning a “not feasible” error, which orchestrator then uses to reschedule elsewhere.
User Interface and Transparency: It's important volunteers feel in control. The Newral client likely has a UI (could be a system tray icon or a simple GUI) showing: - Active tasks and their status (progress, project, time remaining). - Total usage (CPU% graph, etc.) so they see impact. Possibly a pause/resume button to easily snooze all tasks for X time if they need full resources temporarily (e.g., if user starts a game, they might click “snooze Newral for 2 hours”). - Settings panel to adjust the above preferences easily (time schedules, resource limits). - Information on what their contribution has achieved (some motivational stats like credits earned, maybe how many results contributed this week). - Logs or notices if something goes wrong (like “task X crashed” or “download failed, retrying”).
This UI engages the user and also acts as a kill-switch if they ever feel the need to instantly stop computation (which fosters trust—they know they can control it).
Automatic Updates: The client should keep itself updated to the latest version for security and performance improvements. Possibly it checks a server for new versions and auto-updates (with user permission or silently for minor updates). Code signing ensures the update package is legitimate.
Graceful Degradation: If the agent detects heavy load by user apps or high temperature, etc., it might automatically throttle or pause tasks to avoid causing trouble, even beyond what user explicitly set. It’s better to err on side of being too polite resource-wise. A satisfied volunteer will keep running the software; an annoyed one will uninstall it.
Example scenario on client side: - The user is working lightly (browsing). They allowed use while working but with only 2 cores at 50%. The agent ensures tasks use ~50% of total CPU (so maybe 1 core fully busy equivalent across 4 cores). The user doesn’t notice any lag because even if they open something heavy, the low priority ensures their app gets CPU. - At night, the user not active. The agent sees idle state for >5 minutes, so it ramps up to use all 4 cores 100% (per user schedule that says okay between 11pm-7am, for instance). It might even run an extra GPU task at night if available. - The machine starts running a bit hot at 3am due to heavy tasks on all cores. The agent notices CPU temperature beyond threshold and to be safe, it throttles to 80% until temps drop (some do that to avoid constant fan noise). - 7am hits, user moves mouse – agent pauses tasks per schedule. Or maybe they continue if user allowed during use, but let’s say schedule said no computing from 7am to 7pm, so agent suspends all tasks promptly at 7:00 (maybe finishing a critical section or checkpoint first). - The tasks are resumed later or stay suspended until allowed window again.
Comparison to Traditional Systems: This design is very much inspired by successful volunteer computing clients like BOINC, which has proven the importance of user-set constraints and transparency for long-term participation. It is noted that volunteers will engage more if they trust the software not to interfere and if they can see what it’s doing. Running at low priority and respecting user preferences is key[29].
Client Security Recap: Summarizing what ensures a node owner’s machine stays safe: - The tasks can’t (easily) break out of sandbox to do harm. - The tasks are verified as authentic code (no viruses from third parties). - The agent prevents system overload (so it doesn’t crash or become unresponsive due to Newral). - The user’s personal data remains isolated (tasks only operate in designated directories, e.g., the agent might copy needed input files to a sandbox directory and run tasks there, they can’t scan entire disk). - Possibly, Newral tasks by default have no network access (so a malicious code can’t exfiltrate data or attack others; legitimate tasks usually don’t need it since input/output is handled by agent).
Client Reliability: The agent also ensures that results are reliably delivered. If a node temporarily loses internet, the agent caches the result and will upload when connection returns. Similarly, if it can’t reach orchestrator with status due to net issues, it could retry or eventually connect via a fallback (maybe the platform has multiple ingestion endpoints). This way transient net issues or server downtime doesn’t cause tasks to be lost; they just report a bit later. The agent likely stores tasks and results in a local queue if needed.
In conclusion, the client side of Newral is designed to operate unobtrusively and securely on volunteer machines while providing feedback and honoring user-defined constraints[9][25]. This ensures that: - Volunteers feel comfortable running Newral long-term (critical for project success). - The system can gather valuable metrics about resource usage and performance from each node, feeding into scheduling decisions and overall system health monitoring. - Each node’s contributions happen safely and efficiently, converting spare cycles into productive computation without negative side effects for the user. This user-centric approach, combined with robust server-side coordination, makes the Newral platform sustainable and user-friendly.
DAG Task Support and Example Projects (BPSW-Hunter and Carmile)
Newral is not limited to independent parallel tasks; it is built to support complex workflows modeled as Directed Acyclic Graphs (DAGs) of tasks. This capability is crucial for enabling multi-stage computations where tasks have dependencies on the outputs of others. We illustrate this feature and its benefits, and then discuss two example projects – BPSW-Hunter and Carmile – which serve as real-world use cases leveraging Newral’s architecture.
DAG Workflows Support: A DAG in this context is a graph where each node represents a task and edges represent dependencies ("Task A must finish before Task B can start," etc.). Supporting DAGs means Newral can manage entire pipelines: - The orchestrator (or a dedicated workflow manager service) keeps track of which tasks in the DAG are ready to run (all their prerequisites are done) and which are waiting. - Initially, tasks with no dependencies (source nodes in the DAG) are scheduled immediately across the network. - When a task completes, the orchestrator marks its dependents as having one less prerequisite. Once a dependent task's all prerequisites are satisfied, the orchestrator can schedule that task[57]. - This continues until all tasks in the DAG are done and the final outputs are produced.
The DAG can be submitted as a whole by a user/project, possibly described in a JSON or script format listing tasks and dependencies. Newral would parse it, create internal task entries for each node, and then manage execution order.
Key features of Newral’s DAG handling: - Parallelism with Dependencies: Tasks that are independent (no direct or transitive dependency) will run in parallel on different nodes, maximizing concurrency. Only actual dependencies impose ordering. - Dynamic scheduling: As soon as a task finishes, any tasks depending on it are checked. If they're now unblocked, they can be dispatched to a node immediately. This dynamic approach uses Kafka events or DB triggers to promptly kick off next tasks (hence reducing idle time between stages). - Data passing: Outputs of tasks become inputs for dependents. Using the object store, as described earlier, when Task A completes, it may produce some output file which is stored and then its key is attached to Task B (dependent). The agent running Task B will fetch that data. The orchestrator manages passing references (and maybe metadata like size or expected format). In-memory passing is not feasible across nodes, but with fast networks and possibly locality optimization, it's handled. - Fault tolerance in workflows: If a task fails, the orchestrator can decide to retry it (possibly on another node or a couple of times) as it may be a transient node issue. If it ultimately fails, it can mark the whole workflow as failed or skip dependent tasks as they cannot proceed. However, because tasks are typically idempotent or reproducible, orchestrator could re-schedule a failed task to salvage the workflow. - Work stealing for DAGs: If tasks in one part of the DAG finish early and some nodes go idle while others are still computing heavy tasks in another part, the orchestrator can reallocate resources. But typically, DAG tasks are predetermined – nodes finishing early on one branch might just remain idle unless there are other workflows. Newral could concurrently handle multiple DAGs, so idle nodes from one workflow can be assigned tasks from another (multi-tenant usage). - Coordination overhead: The orchestrator uses events to manage DAG progression. It needs to ensure all dependencies are noted and all completions trigger the next steps. This adds some overhead (bookkeeping of DAG state), but it’s manageable via DB or in-memory structure plus Kafka triggers. Many frameworks (Airflow, Dask, etc.) do this at a higher software layer; Newral would have that capability as part of core orchestrator or an integrated workflow service.
Benefits of DAG Support: - Complex Application Support: Many real-world computations are multi-step. E.g., data preprocessing -> computation -> postprocessing. Or iterative algorithms, or conditional workflows. Newral can support these seamlessly, whereas simpler volunteer systems might require manual splitting by the user. - Resource Optimization: Some tasks may be heavy CPU, others heavy I/O or memory. If orchestrator knows the DAG, it can plan to schedule tasks on appropriate nodes for that stage (e.g., memory-heavy tasks on high-RAM nodes, CPU tasks anywhere). - Faster End-to-End Time: Instead of waiting for an entire stage to complete before starting the next (barrier synchronization, like MapReduce does), as soon as individual tasks finish, their dependents can start. This pipelining reduces total workflow time. - Reusable sub-results: If the same subtask’s result is needed by multiple dependents, orchestrator ensures it's computed once and then reused by all dependents (each dependent downloads the same object). For example, if three tasks need output of A, A runs once and all three fetch its output – vs running A three times. DAG makes that clear to orchestrator.
Now, let's illustrate DAG support with the example projects:
Project BPSW-Hunter:
Baillie–PSW primality test (BPSW) is a probabilistic primality test with no known counterexamples – no composite number is known to pass it as “prime.” BPSW-Hunter is a hypothetical project on Newral that aims to either find a counterexample or push the bounds further (e.g., verify BPSW for numbers up to a very large range). It's inspired by the unsolved problem: “Is there a composite that passes Baillie–PSW?”[58].
Nature of Computation: This project likely involves testing many numbers for primality using the BPSW test: - It might involve generating candidate numbers (perhaps certain forms believed likely to be counterexamples, or just sequentially large numbers). - For each candidate, performing a BPSW test (which includes a Miller-Rabin base-2 test and a Lucas test). - If any candidate passes the test (despite being composite), that’s a find; otherwise, verifying none up to some huge bound do.
Using Newral: - Parallel Independent Tasks: At first glance, testing different numbers is embarrassingly parallel. The orchestrator can give each node a range or a set of numbers to test. Each number’s test is one task (or a batch of numbers per task). This could be done without DAG complexities because each test is independent. - But there's DAG potential: If the process is more complicated: for instance, if a number passes the BPSW test (suspected pseudoprime), the project might then require a verification step, like a more intensive deterministic primality proof or a factorization to confirm it’s composite. That would be a follow-up task dependent on the first. - For example, a DAG could be: Task A = “run BPSW test on candidate N”; if A's result = “N passes BPSW (likely prime)”, then Task B = “run a deterministic primality proof or attempt to factor N”. This is a two-stage DAG: B depends on A. - This ensures any promising candidate is double-checked by a more time-consuming method, but only when needed. The orchestrator will only spawn B when A indicates a possible counterexample. - If the test fails (N is found composite by BPSW itself), no B needed, or B can be a trivial "skip".
Workload and Newral scale: BPSW-Hunter could involve extremely large numbers (64-bit and beyond). It's computationally heavy – though BPSW is relatively fast per test, the space of candidates might be enormous. It's perfect for distributed brute force scanning. Newral’s advantages: - It can test numbers in parallel on many nodes. If each test is quick, orchestrator overhead might be significant, so bundling multiple tests per task might be considered (one task tests, say, 1000 numbers in a loop, to amortize overhead). The project designers decide that. - Adaptive strategy: Using reputation: nodes that often correctly identify composites vs primes become trusted to not need double-check, whereas a new node might have its outputs cross-verified occasionally (though primality testing is deterministic/probabilistic rather than subjective, so trust is less an issue than in open-ended tasks).
Outcome verification: If a node reports a pseudoprime found, that’s a big deal. The platform would likely replicate that test on other nodes (especially on high-rep nodes) to be sure (since if a malicious node falsely claims "hey I found a counterexample" but it actually didn't test properly, it could mislead science). So multiple nodes would re-run that BPSW test and then the follow-up proof. Only if confirmed by independent validation would the result be accepted (this is about scientific caution and also trust in distributed results).
Current state: As background, it's known that no composite below 2^64 passes Baillie–PSW[32]. BPSW-Hunter might push beyond 2^64, potentially into 128-bit or 256-bit territory where checking every number isn't feasible, but maybe targeted search or random search is done. The project might have volunteer nodes checking random large composites for being pseudoprime to the tests. A bounty exists ($30 by Pomerance, and $2000 for an extended test)[33], which shows the interest in such discovery.
Project Carmile:
Likely derived from Carmichael numbers (a Carmichael number is a composite that passes Fermat's little theorem primality test for all bases coprime to it)[59]. Carmile might be a project to discover new Carmichael numbers or study their distribution. Possibly "Carmile" pun on Carmichael minus "cha" (just speculation on name). Carmichael numbers are rare and grow sparse but infinitely many exist[35]. Searching for large Carmichael numbers is computationally challenging.
Nature of Carmichael search: To find Carmichael numbers, one approach is: - Generate candidates that satisfy Korselt’s criterion (necessary and sufficient condition for Carmichael numbers): n is Carmichael if n is composite, square-free, and for every prime divisor p of n, p-1 divides n-1[60]. - So often the approach is to generate sets of primes that satisfy certain congruences to meet p-1|n-1 for each. - It becomes a combinatorial search: choose primes p,q,r,... such that for each, (p-1)|(pq...r - 1). This can be heavy and often done via backtracking or heuristic search.
Using Newral: - This can naturally be a multi-stage process: 1. Generate prime combinations (candidates): This could be a task: pick some base primes and form n = pqr. Check easy conditions (like maybe all primes are distinct (square-free), small divisibility tests). 2. Test each candidate for Carmichael property: This is a second task (dependent on output of generation) that does a thorough check (maybe factoring or verifying Korselt's criterion explicitly). Possibly test by checking a^n ≡ a (mod n) for some bases as a quick filter, or do a full check of all base primes dividing n-1 conditions. 3. Validate found Carmichael numbers further (like double-check prime factors indeed satisfy all conditions) and perhaps do something with them (store or analyze). - This is naturally a DAG: generation -> test -> (maybe additional analysis). - We might spawn many generation tasks in parallel exploring different prime combinations (they might randomly or systematically explore the search space). - Each generation task outputs candidate numbers (maybe hundreds) as potential Carmichaels. - Those outputs are then fed into test tasks (that is a map-reduce-ish DAG: generation is like 'map' producing candidates, testing is like 'reduce' filtering them). Actually, not exactly reduce, more like another map on output list. - Or generation tasks could directly spawn test sub-tasks for each candidate as they find them (even more granular pipeline).
However, it might be more efficient to incorporate testing in the generation task to immediately discard non-Carmichaels. It depends how they partition work: - They might fix 2 primes and search for a third that satisfies conditions, etc. That can be done by subdividing search space among nodes.
Computational difficulty: Carmichael number density is very low. Searching for new ones often involves exploring large search spaces of primes. It's a good fit for distributed computing because it's embarrassingly parallel: test many combinations concurrently. Historically, new Carmichael numbers (with many prime factors) have been found with heavy computation.
For example, computing all Carmichael numbers up to 10^21 found ~20 billion of them up to that range[36], which indicates how computation heavy it is (though likely they used theoretical insights, not brute force).
Newral's advantage for Carmile: - It can try many combinations in parallel. Because there's no strong dependency between exploring different regions of search, except maybe if one approach fails and they adjust strategy (which could be done between runs, not within a single DAG). - If a particular combination yields a Carmichael number, that result might be interesting to cross-verify by other means (like factoring it to double-check all factors, as an extra guarantee of correctness). - A found Carmichael number is easy to verify once prime factors are known (just check criteria). - If a node claims a Carmichael number find, again multi-validation kicks in. It’s crucial because false positives must be eliminated (Carmichael definition requires checking a condition for all primes dividing n, which the algorithm should ensure, but double-check won't hurt).
DAG for Carmile Example: 1. Task A: Generate candidate n from primes (p,q,r,...). (Perhaps A outputs n and its prime factors or just n). 2. Task B: Given n, verify if n is Carmichael (ensures square-free, does Fermat test maybe for few bases as quick check, and cross-check each prime factor’s p-1 divides n-1). 3. Task C (if Carmichael): Additional output processing (maybe store it to a list, run some classification on it, or double-check via alternate method). 4. Many A tasks run, each triggers a B (or multiple B if multiple candidates). B that succeed trigger C. - The orchestrator ensures tasks B only run for outputs of A tasks, and tasks C only run for outputs of B that are true Carmichaels (dependency chain). - This pipeline keeps all nodes busy: while some generate, others verify earlier ones.
Support Projects concurrently: Newral can run BPSW-Hunter and Carmile concurrently on the same infrastructure. Nodes might request tasks from whichever projects they opted into, etc. Or orchestrator timeshares nodes among projects based on some priorities or user allocations. The platform supports multiple DAGs (multi-tenant) because tasks carry project tags and orchestrator can isolate them logically.
Project Outcomes and Reputation: - Both projects, being mathematical, have well-defined correct outcomes (find numbers with certain properties). It's not subjective, so verifying correctness of results is straightforward: either the number indeed has property or not. That helps in reputation calculation— nodes essentially either deliver correct finds or they don't find anything or incorrectly label something as find, which validation catches and marks them wrong. - For example, if a node in Carmile outputs "n is a Carmichael" but it's not, the validation (like rechecking conditions or factoring n) will catch the error[59]. That node’s reputation would drop for a false claim, discouraging spam results.
Demonstrating Newral’s Capabilities: These projects act as flagship uses: - Massive Parallel Crunching (BPSW-Hunter): Showcases raw computing power on independent tasks, scaling to test astronomically many cases and pushing boundaries (like verifying something up to 2^64 and beyond with volunteer support). - Complex Workflow with Computation and Search (Carmile): Showcases handling of more complex logic and multi-step processes on the platform, demonstrating that Newral can orchestrate not just simple tasks but also structured searches that require coordination, intermediate data passing, and conditional execution.
In both, the role of Newral is to dramatically accelerate research by dividing the work among volunteers worldwide, with the confidence that results are verified (so discoveries can be trusted as real)[15], and the efficiency that tasks are executed in parallel and as soon as they’re ready (minimizing waiting idle). Without such a platform, these projects might run for years on single machines or small clusters, whereas with Newral, they harness potentially orders of magnitude more compute.
Furthermore, these projects can directly benefit from Newral’s trust and reward system: - They can attract volunteers by advertising the scientific importance (perhaps credit points and recognition for contributing to finding a historic counterexample or a rare Carmichael number). - Volunteers know their computer time isn’t wasted due to robust validation; any false rumor of discovery will be caught by replication, meaning the project outcomes remain credible. - If a volunteer’s node is lucky enough to find, say, the first known Baillie-PSW pseudoprime (counterexample), that user would gain significant recognition (and maybe a share of that $30 prize – albeit small, but bragging rights huge!). Newral’s logs could prove which node found it first etc.
In summary, supporting DAG tasks allows Newral to handle the full complexity of distributed computations needed by advanced projects like BPSW-Hunter and Carmile. The platform provides the necessary flexibility and structure: - It manages dependencies and multi-stage processes elegantly. - It provides the computational muscle to cover immense search spaces in parallel. - It ensures accuracy of results via cross-verification (critical in mathematical projects where one incorrect result can propagate a false claim). - It fosters a community approach where volunteers tackle cutting-edge problems (unsolved in number theory in these cases) with the reassurance of a robust infrastructure supporting them.
These example projects underscore how Newral is suited not just for trivial distributed tasks, but for challenging, unsolved scientific problems, by orchestrating complex workflows at scale and engaging a broad volunteer network to collectively push the boundaries of knowledge.
Technology Stack
Newral is implemented using a modern technology stack designed for scalability, reliability, and performance. The stack was chosen to leverage proven frameworks and tools that align with the platform’s microservice, distributed nature, and to facilitate the integration of AI components. Here is an overview of the key technologies and components in use:
•	Programming Languages:
•	Python: Used for high-level orchestration logic, especially where integration with machine learning is required. The AI Orchestrator modes and scheduling algorithms (e.g., training and applying ML models for decision-making) are conveniently implemented in Python due to its rich ML ecosystem (libraries like TensorFlow/PyTorch for training, scikit-learn for simpler models). Also, Python with frameworks like FastAPI or Flask is used for the API Gateway service, providing REST/GraphQL endpoints to users and projects.
•	Java/Scala: The choice for services where performance and concurrency matter at scale (and where JVM libraries excel). For instance, parts of the orchestrator or the workflow manager might be written in Scala using Akka or Spring Boot for robust multithreading. Additionally, if the team uses Kafka Streams for any real-time processing or if they embed business logic in stream processing, Scala/Java is natural.
•	Go (Golang): Utilized for microservices that benefit from low-footprint and high concurrency with simplicity. For example, the client agent or some lightweight services (like the reputation service, which just applies straightforward rules) might be written in Go for efficiency and ease of deployment. Go’s static binaries ease distribution (especially for clients on multiple OS).
•	C/C++: These come into play for performance-critical computation libraries. For example, the BPSW primality test or other numeric heavy routines might use highly optimized C/C++ libraries (perhaps even assembly for big integer operations). Many such libraries exist (e.g., GMP for big integers in C). If tasks require heavy math, those tasks’ code might be in C++ for speed, even if orchestrated by Python. Also, any low-level integration (like a custom sandboxing mechanism on client side) might use C++ to interface with OS features.
•	Rust: Possibly used in place of C++ for systems programming tasks that demand safety (e.g., sandboxing, certain parts of client or even a high-performance service). Rust could implement, say, a safe sandbox runner that executes untrusted code, benefiting from Rust’s memory safety.
The microservice architecture allows a polyglot approach[37] – each component uses the best language for its job, and they communicate via network APIs or Kafka, making language boundaries irrelevant to overall integration.
•	Microservices and Containerization:
•	Each service is packaged as a Docker container. Docker ensures consistency across environments (dev, test, prod) and eases scaling on Kubernetes.
•	Kubernetes (K8s) is used to deploy and manage these containers in the cloud or on a cluster[6]. It handles service discovery, scaling (through Horizontal Pod Autoscalers if needed), and self-healing (restarting crashed services).
•	K8s also manages configuration (ConfigMaps/Secrets for DB credentials, etc.) and secure service-to-service communication (possibly through mTLS within the cluster if desired). It simplifies rolling updates of services (so new orchestrator versions can be deployed with zero downtime).
•	The microservices likely include: Orchestrator Service, Validation/Trust Service, API Gateway Service, Monitoring/Metric Collector Service, etc., each running in its own container.
•	Database:
•	PostgreSQL serves as the core relational database. We use advanced features like:
o	JSONB columns to store flexible data (like DAG definitions or task parameters that vary by project).
o	Indexes and partitioning to handle large tables such as tasks (which can number in millions or more). Partitioning by date or project ID might keep performance high.
o	Stored procedures could implement some logic like awarding credits transactionally with updating tasks – though most logic remains in app code.
•	The choice of Postgres is due to its robustness and strong community support. It easily handles the moderate relational workload and ensures transactional integrity (for example, updating a task state and a node’s credit in one transaction)[55].
•	For scaling reads, we can add read replicas (useful if there's heavy API querying for status and the single master becomes a bottleneck). For write scaling, tasks can be partitioned across multiple Postgres instances by project (if needed in far future), but MVP likely will be fine with one decent Postgres server.
•	Object Storage:
•	MinIO is a popular self-hosted S3-compatible object storage solution. It can run on the same Kubernetes cluster using local or cloud volumes, offering high performance for moderate scale (it’s well-suited for up to petabytes).
•	If deploying on a specific cloud, using native storage like AWS S3, Google Cloud Storage, or Azure Blob Storage is an option with minimal changes (just endpoint differences), thanks to compatibility.
•	Object storage is accessed via the standard S3 API (HTTP). The client nodes use an S3 library or HTTP requests to download/upload. The orchestrator may generate pre-signed URLs for nodes to use (so node can download without needing long-lived credentials, if security concerns).
•	Backups: If using MinIO, we'd implement bucket replication to another MinIO or periodic snapshots (MinIO can mirror to other S3 buckets). If using cloud, the cloud itself often has durability and versioning features.
•	Encryption: Sensitive data might be stored encrypted at rest. Many object stores handle encryption. If needed, we can encrypt certain objects on client side too, but that may not be necessary for computational results.
•	Event Streaming & Messaging:
•	Apache Kafka orchestrates events as detailed. We likely use the Confluent Platform or a hosted Kafka (like AWS MSK, Aiven, etc.) for convenience, which includes Schema Registry.
•	Schema Registry (Avro/Protobuf): By using Avro or Protobuf schemas for Kafka messages[56], we ensure type consistency. For example, a TaskCompleted event has a schema with fields (task_id, node_id, result_key, timestamp, status, etc.). All producers and consumers share this, preventing bugs from mismatched expectations.
•	Kafka Connectors: We might use Connect to stream important events or aggregated logs to monitoring systems or long-term storage. For example, pushing a summary of daily credit granted into a database for stats, or feeding real-time system metrics into a dashboard.
•	KSQL / Kafka Streams: Possibly used to derive some live metrics. E.g., count tasks completed per minute sliding window for monitoring, or detect if a node’s results disagree with others (though such logic might be easier in orchestrator code).
•	Zookeeper: If using older Kafka, Zookeeper runs for metadata management (Kubernetes will manage it as a cluster). In future, Kafka may use its internal quorum mode.
•	AI/ML Components:
•	PyTorch or TensorFlow: one of these for developing any learning model the orchestrator might use (e.g., to predict task run times on certain node types, or to classify nodes as likely to churn or be malicious). For MVP, perhaps simple heuristics, but as “Next-Generation” tagline suggests, eventually training an RL or supervised model could be done.
•	Scikit-learn/XGBoost: Possibly for simpler predictive analytics (like linear regression to predict durations, or anomaly detection on node performance metrics).
•	Pandas/Numpy: used in offline analysis (the orchestrator could record logs, and an offline training job uses Pandas to crunch historical data and retrain an AI model).
•	AutoML frameworks: if partial automation is needed to tune models or if implementing something like an Auto-scheduler that tries multiple strategies. However, likely not needed beyond core frameworks.
•	Integration: The models might be trained periodically and then loaded into the orchestrator for inference (for example, a model that given task features and node features predicts completion time, used in Advisory mode suggestions). Tools like ONNX could allow using a model across languages if needed (e.g., train in Python, run in Java orchestrator via ONNX runtime).
•	Web/API Layer:
•	FastAPI (Python) or Spring Boot (Java) for the API Gateway. FastAPI is a good choice as it’s high-performance (built on Uvicorn ASGI, can do tens of thousands of requests/sec) and easy to write schemas and docs for. It also can integrate with Pydantic for request/response models, which is useful.
•	GraphQL could be offered (maybe using Ariadne or Graphene in Python, or Spring GraphQL in Java) for clients who want flexible queries (like retrieving complex nested info about tasks and nodes in one query).
•	Authentication: Likely using JWT (JSON Web Tokens) issued via an auth service (possibly integrated with something like Keycloak or a simple custom login for MVP).
•	Admins might have a UI with more powers; volunteers might have minimal login or just rely on client for contribution.
•	The API gateway would also handle static serving of any web dashboard (could be a React single-page app).
•	Front-end (Dashboard):
•	Perhaps built with React + a UI toolkit (Material-UI, etc.) for any web interface to view system status or user stats. This would communicate with the gateway APIs.
•	Not critical for computing, but important for user engagement, monitoring results, etc.
•	For volunteers, maybe the main interface is the client GUI on their machine, but a web component could allow them to log into an account and see their contributions from anywhere.
•	Monitoring and Logging:
•	Prometheus collects metrics from services and perhaps from nodes (though for node hardware metrics, those are reported through orchestrator anyway). Each microservice exposes a /metrics endpoint (common in K8s setups) and Prometheus scrapes them. E.g., orchestrator might expose number of tasks waiting, tasks running, etc., Kafka exposes consumer lag, etc.
•	Grafana to visualize metrics in dashboards. For instance, an admin dashboard showing number of active nodes, tasks completed per hour, average task duration per project, etc. This is vital for tuning the platform and seeing if all parts function well.
•	ELK Stack (Elasticsearch, Logstash, Kibana) or EFK (Fluentd instead of Logstash) for logging. All service logs (and possibly important node logs) are aggregated to Elasticsearch. Kibana or Grafana's Loki could be used to search logs. This is helpful for debugging issues (like if a specific node consistently errors out tasks, logs might show why).
•	Alerting: Setup alerts via Prometheus Alertmanager or others. e.g., if orchestrator queue length > X, or no heartbeat from >10% of nodes, etc. This ensures prompt response to problems.
•	Security:
•	Transport Security: All network communication is over TLS. For client <-> server, we can use HTTPS and wss (for any websockets if needed). For internal microservice calls, if within cluster, it might be plain or also TLS with service mesh like Istio for mTLS if desired. However, Kafka by default might not be TLS; we’d configure it with SASL_SSL for production.
•	Authentication & Authorization: Possibly a Keycloak or Auth0 to manage user accounts (especially if broad volunteer login to see stats, etc.). It can provide OAuth tokens for API access.
o	Or simpler, since volunteers typically sign up to projects separately, we could embed an identity inside the client (like a generated node ID with a secret).
•	Node Registration: The first time a client connects, orchestrator might require a token (maybe user gets from website) or uses client’s provided credentials (like BOINC uses an account manager with login). Some method to avoid unauthorized join. If completely open, one could spam nodes which is a security risk.
•	Encryption: Data at rest in DB (could use pgcrypto or TDE if sensitive info is stored like user email), data at rest in object storage (S3 encryption). But computational results might not be sensitive (unless some projects use proprietary or personal data).
•	Secret Management: Kubernetes secrets for DB passwords, MinIO keys, etc., integrated. If needed, use Vault for dynamic secret provisioning.
•	Client Application:
•	Possibly packaged with Electron if a fancy cross-platform GUI is needed (like BOINC Manager). But an Electron app might be heavy; a simpler approach is to have a native tray application per OS.
o	On Windows, could be a .NET or Qt app, on Linux maybe Qt or just CLI+web UI, on Mac maybe native or cross-compiled Qt.
o	Alternatively, use WxWidgets/Qt in C++ or Tkinter/PyQt in Python to have one codebase (Python's PyInstaller to freeze it for each OS).
•	The core computing part likely in C++ or Rust for performance (especially if tasks are run as threads in the client, they want minimal overhead).
•	The agent might run as a background service and the GUI just communicates with it (like BOINC does: a daemon plus a GUI that connects via RPC to the daemon). This is good design: the service can run at boot and do work even without user logging in, and the GUI can be optional for control.
•	Communications between client and server use protocols like gRPC or HTTPS as discussed, and often asynchronous (client polls for tasks or gets tasks via Kafka).
•	The client uses OpenSSL or similar for any crypto, and OS-specific APIs for things like detecting user activity or battery status.
•	Credit and Reward System:
•	Possibly separate microservice or part of trust service. Could incorporate a small blockchain component if they considered using a crypto token as reward (but likely not MVP). But maybe simply store credits in Postgres and maybe allow conversion to some form (like tokens to withdraw from a central wallet).
•	Notably, projects like Golem built a blockchain for payments. If Newral wanted that, it could integrate with Ethereum or similar. But that adds huge complexity and is not explicitly requested here, so probably outside MVP scope. For now, a centralized credit ledger is fine.
•	Roadmap Tech Additions (post-MVP):
•	Could integrate TensorFlow Serving or ONNX Runtime if heavy AI decision-making is moved out of orchestrator process (like a separate service that given state returns an AI decision).
•	Distributed computing frameworks: If at some point tasks themselves require multi-node coordination (like MPI-style tasks), Newral might integrate something like an MPI cluster manager or provide that as a mode. But likely it stays in realm of independent tasks.
•	P2P distribution: Perhaps using peer-to-peer protocols for distributing large data among nodes (like if 100 nodes need same 1GB file, maybe use IPFS or BitTorrent style to share among them rather than all from central). That could dramatically reduce load on central storage. This wasn’t initially described but is a tech that can overlay for efficiency. Some volunteer projects did this with varying success.
•	GPU support: Ensure the agent can detect GPUs and orchestrator can assign GPU tasks appropriately. That might involve using frameworks like CUDA, OpenCL or higher-level libs depending on tasks. The technology stack would then include NVIDIA’s CUDA toolkit for any GPU tasks and scheduling logic to ensure only one heavy GPU task per GPU, etc.
The selected stack is intended to be robust and future-proof: - It uses industry-standard tools with strong communities (Postgres, Kafka, Kubernetes, etc.), so scaling and maintaining them is well understood. - It emphasizes modularity so each piece can be upgraded or replaced (for example, if in future a faster message queue is needed, or a different DB for certain data). - It aligns with the platform’s goals: microservices, AI integration, cross-platform client distribution, security, and high performance.
By leveraging this tech stack, the Newral platform can be delivered as a cloud-native application and also deploy a rich client to volunteers, combining the best of cloud computing and distributed volunteer computing approaches.
MVP Roadmap
Developing a complex platform like Newral requires breaking down the work into phases. Below is a roadmap outlining the steps towards a Minimum Viable Product (MVP), along with subsequent milestones. Each phase produces a working subset of features, progressively building up to the full platform capabilities. Emphasis is placed on delivering a functional system early and then iteratively enhancing it.
Phase 1: Core Architecture & Basic Job Execution (MVP Step 1) 1. Foundational Setup (Month 0-1): - Set up version control, CI/CD pipeline, and basic project structure for microservices. - Deploy base infrastructure: get a Kubernetes cluster running (or Docker Compose for early dev), spin up PostgreSQL and Kafka in the environment. - Implement a simple "Hello World" service that spans a client and orchestrator: e.g., client requests a dummy task, orchestrator assigns it, client performs trivial calc, returns result. This validates the basic communication (API or Kafka) path.
1.	Task Distribution & Execution (Month 2-3):
2.	Orchestrator Service (initial): Develop the minimal orchestrator that can accept tasks (via a simple API or even reading from DB for now), place them in a queue, and assign them to connected nodes. At this stage, scheduling can be FIFO or round-robin with no AI logic (AI Off mode essentially).
3.	Client Agent (initial): Develop a basic cross-platform client that can:
o	Connect/register with orchestrator.
o	Poll or subscribe for tasks.
o	Execute a task (could be a simple CPU-bound function like computing a checksum or sleeping to simulate work).
o	Return result status back to orchestrator.
4.	Networking: Use straightforward REST calls or WebSockets for assignment/heartbeat. Possibly use Kafka if ready, but MVP can start with simpler approach to reduce moving parts.
5.	Achieve first end-to-end demonstration: submit a test task through the API (or insert into DB), orchestrator sends to client, client performs and returns result, orchestrator marks it done[15]. This proves the core job execution loop.
6.	Basic Monitoring & Persistence (Month 3):
7.	Persist task and node info in PostgreSQL (rather than in-memory), so that if orchestrator restarts, it can recover state.
8.	Implement rudimentary logging and a way to view status (perhaps a CLI or simple web page showing tasks in queue/running/done).
9.	Integrate Prometheus to start tracking metrics (like number of tasks processed) for internal testing.
Phase 2: Task Verification & Trust Framework (MVP Step 2) 4. Result Validation Mechanism (Month 4): - Develop the validation logic: For now, simplest approach is redundancy. Mark certain tasks to be executed by 2 distinct nodes (or same node twice sequentially for testing)[15]. - Orchestrator picks two nodes for each task, compares results when both arrive. - Implement logic to mark results as verified if they match, or flag discrepancy if not. - Basic action on mismatch: log it and maybe reassign to a third node (majority wins) – automated or manual for now. - This sets the stage for trust – proving we can catch incorrect results.
1.	Reputation & Trust Model (Month 5):
2.	Create a reputation score field for nodes in the DB. Define simple rules: e.g., +1 for each verified correct result, -5 for each incorrect result (tunable).
3.	Develop a Trust service or incorporate into orchestrator: after each task verification, update involved nodes' rep scores[7].
4.	Use rep in scheduling: e.g., orchestrator may avoid giving big tasks to nodes with rep below X or may decide how many replications to use based on rep (if rep < Y, do double computing).
5.	UI/Logging: let node owners see their rep (maybe just in a log or a proto-web interface).
6.	Now we have a basic self-healing network: good nodes gradually stop being double-checked, bad nodes are caught and their results always double-checked or even blacklisted if rep too low.
7.	Security and Sandbox in Client (Month 5-6):
8.	Implement code signing verification: sign the dummy task executable or script with a test key, client verifies before running[8].
9.	Add sandboxing: perhaps for MVP, run task in a separate process with reduced privileges (for Windows, maybe run as normal user if agent is service; for Linux, maybe use user namespaces or a simple cgroup jail).
10.	It might not be the full VM/Container isolation yet, but at least ensure the working directory is separate and no dangerous system calls (this could be limited, it's complex to fully sandbox without containers which might come later).
11.	Basic user preference on client: allow user to set a CPU usage cap or time schedule via a config file or simple GUI, and make sure agent respects it (test by seeing it pause tasks during disallowed times).
12.	These steps ensure that by MVP, the solution is safe and configurable for volunteer use.
Phase 3: Microservices Refinement & Full Pipeline (MVP Step 3) 7. Kafka Integration & Event-Driven Orchestration (Month 6-7): - Switch the communication to use Kafka topics for task assignment and completion instead of (or in addition to) REST polling[4]. - This likely means: orchestrator produces to a tasks topic, clients consume their assignments from it; clients produce task_results events, orchestrator consumes them. - Ensure ordering and offset management, using consumer groups properly. - This might come earlier if the team is comfortable, but putting it here allows first getting logic right then optimizing comms. - Test that scaling orchestrator (multiple instances) works with Kafka (e.g., only one should pick up a specific task assignment event, which can be achieved by proper partition key or using Kafka as a work queue).
1.	DAG Workflow Support (Month 7-8):
2.	Enhance the orchestrator to accept not just single tasks but a DAG description (maybe via an API endpoint where a user posts a DAG in JSON, listing tasks with their dependencies).
3.	Store DAG and task nodes in DB with dependency pointers. Implement logic that when a task finishes, it checks dependent tasks and if all preds done, triggers them[57].
4.	Possibly leverage Kafka: task completion events could be consumed by a workflow manager component that then enqueues dependent tasks.
5.	Start with a simple linear chain or small graph to test. E.g., tasks A -> B -> C (C depends on B, B on A). Ensure they run in correct order across possibly different nodes and outputs of A go to B, etc.
6.	Introduce object storage usage properly now: tasks produce actual output files (even small dummy ones) and next tasks retrieve them[3]. This tests the data passing.
7.	Confirm that multi-step tasks complete end-to-end on the platform.
8.	Example Project Implementation (Month 8):
9.	As a demonstration, implement a scaled-down version of a real use-case, e.g., a mini BPSW-Hunter:
o	Have tasks that test a range of numbers for primality. Use a simple primality test method (for development) or integrate an existing library for Miller-Rabin.
o	This will produce some pseudoprimes maybe (we could simulate one artificially for demonstration). Show that if a pseudoprime is found, the platform catches it and maybe triggers a follow-up task (like factorization).
10.	Or a mini Carmile:
o	Have generation tasks listing a few candidate composites, then check tasks verifying Carmichael property.
11.	The goal is to validate the platform with something resembling the final intended workload, to flush out performance or logic issues. It also serves as a good MVP demonstration to stakeholders (the platform doing something meaningful).
12.	User Interface & Control (Month 9):
o	Develop a basic volunteer UI (if not done, do now): a simple GUI for client that shows current task, resource usage, start/pause, etc. Could be minimal but should exist for MVP if volunteers are expected to run it.
o	Develop a basic web dashboard for admins/project owners:
o	Show active nodes, their reputations, tasks in progress, etc., possibly using Grafana or a custom page.
o	Show results being found (like if a pseudoprime or Carmichael is found, highlight it).
o	This isn't core computing but important for usability and transparency, which is part of delivering an MVP that people can interact with.
Phase 4: MVP Completion and Testing (Month 9-10): 11. Testing & Hardening (Month 9): - Conduct extensive testing: - Unit tests for each component (e.g., simulate a task and ensure orchestrator logic works, trust updates properly). - Integration tests on a small cluster (maybe bring up 3-5 client nodes on VMs or containers, run a bunch of sample tasks). - Induce failures: kill a client mid-task (should orchestrator reassign? Or detect timeout?). Kill orchestrator and restart (should recover from DB and Kafka log). - Security testing: attempt to send bad data from a fake client, ensure it's rejected or doesn't compromise server. Attempt to run a malicious code in client sandbox (ensuring it can't break out). - Performance test: ramp up number of tasks and nodes (maybe simulate 100 nodes with threads) to see if orchestrator & Kafka handle the volume, tune if needed (increase partitions, etc.). - Fix any bugs, optimize hotspots (e.g., maybe add caching or better queries if DB is slow on certain operations).
1.	MVP Release (Month 10):
o	Prepare documentation:
o	Developer docs (architecture, how to add a new service or modify ML model).
o	Deployment instructions (how to set up the platform on cluster, how to run client).
o	User guide for volunteers (how to install client, what settings mean).
o	API documentation for project owners (how to submit tasks or DAGs via API).
o	If applicable, deploy MVP on a staging environment and then move to production environment for initial users (maybe start with a friendly user group, like internal testers or a small community).
o	Collect feedback from initial users and address immediate issues (quick patch releases).
o	The platform at MVP stage should be fully functional: volunteers can join and contribute, tasks (both independent and simple workflows) can be submitted and completed, results are verified, and basic management is possible.
With this roadmap, by the end of Phase 4, Newral MVP will have: - Microservice core (orchestrator, etc.) stable and scaled for initial user base. - Client agent available on major OS, performing tasks without interfering with users. - Task distribution, DAG, validation, trust, and basic AI-off scheduling working[15]. - Safety nets (reputation-based checks, sandboxing, encryption) ensuring reliability and security. - UI components for usability (though perhaps rudimentary).
Beyond MVP (Phase 5+ tentative): While not required in MVP, it's good to outline: - Phase 5: AI Integration (Post-MVP): Once baseline is stable, start experimenting with AI modes (Advisory, Partial, Full). This would involve collecting data from MVP usage to train models (e.g., performance predictors). Then implement Advisory suggestions UI, etc. - Phase 6: Enhanced Incentives: Integrate a more elaborate reward system (like point leaderboards on a website, or tokens if desired). - Phase 7: Scalability and Performance Tuning: When user base grows, refine components (shard DB if needed, optimize Kafka partitions, etc.) and perhaps implement peer-to-peer data distribution to offload central storage for huge projects. - Phase 8: Community and Project Tools: Provide project owners with easier tools to create tasks/DAGs (maybe a web interface or SDK), and volunteers with community features (forums, etc. integrated or separate). - Phase 9: Full AI Orchestration: After lots of data and testing, gradually enable Partial and then Full AI mode in production for scheduling. Monitor outcomes carefully, adding overrides or failsafes (like a “big red button” to fall back to AI Off if something goes wrong).
This staged approach ensures that at MVP, stakeholders get a working platform demonstrating Newral’s key features – distributed computing across nodes with verification and basic orchestration – and from there, further sophistication (like AI scheduling, advanced security, or large-scale optimizations) can be layered on with confidence. Each milestone builds upon previous ones in functionality and reliability, in line with best practices for delivering a complex system.
________________________________________
[1] [7] [11] [16] [38] [46] [47] [48] [50] [51] [52] Trust Management in Distributed Systems - GeeksforGeeks
https://www.geeksforgeeks.org/system-design/trust-management-in-distributed-systems/
[2] [4] [6] [37] [40] [55] Microservices Architecture Style - Azure Architecture Center | Microsoft Learn
https://learn.microsoft.com/en-us/azure/architecture/guide/architecture-styles/microservices
[3] How We Built RisingWave on S3: A Deep Dive into S3-as-primary-storage Architecture - RisingWave: Event Streaming Platform For Agents, Apps, and Analytics
https://risingwave.com/blog/how-we-built-risingwave-on-s3-a-deep-dive-into-s3-as-primary-storage-architecture/
[5] [18] [19] [20] [21] [39] [56] Event Driven Architecture and Kafka Explained: Pros and Cons
https://www.prodyna.com/insights/event-driven-architecture-and-kafka
[8] [9] [10] [15] [17] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [41] [42] BOINC a Platform for Volunteer Computing | PDF | Virtual Machine | Thread (Computing)
https://www.scribd.com/document/960825480/BOINC-a-Platform-for-Volunteer-Computing
[12] [14] Introducing Golem Network’s Reputation System
https://blog.golem.network/introducing-golem-networks-reputation-system/
[13] Golem Network in Blockchain - GeeksforGeeks
https://www.geeksforgeeks.org/computer-networks/golem-network-in-blockchain/
[32] [33] [58] Baillie–PSW primality test - Wikipedia
https://en.wikipedia.org/wiki/Baillie%E2%80%93PSW_primality_test
[34] [35] [36] [59] [60] Carmichael number - Wikipedia
https://en.wikipedia.org/wiki/Carmichael_number
[43] What is AI Orchestration? 21+ Tools to Consider in 2025
https://akka.io/blog/ai-orchestration-tools
[44] AI Ecosystem Orchestrator: How to Keep Your AI Agents Working ...
https://addepto.com/blog/ai-ecosystem-orchestrator-how-to-keep-your-ai-agents-working-together/
[45] AI Orchestration Guide: Build Scalable, Intelligent Workflows with ...
https://globalnodes.tech/blog/ai-orchestration-guide/
[49] Trust-based collection of information in distributed reputation networks
https://dl.acm.org/doi/10.1145/2695664.2695868
[53] [PDF] BOINC: A Platform for Volunteer Computing 1. Introduction - arXiv
https://arxiv.org/pdf/1903.01699
[54] Quorum, Replication. - SETI@home
https://setiathome.ssl.berkeley.edu/beta/forum_thread.php?id=1415&postid=35270
[57] How Apache Airflow's task dependency and scheduling mechanism ...
https://leonidasgorgo.medium.com/how-apache-airflows-task-dependency-and-scheduling-mechanism-work-714b55548cc0
